<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>DiET | lif323</title>
<meta name=keywords content><meta name=description content="A major issue with post-hoc explainability is its inability to faithfully represent the model&rsquo;s underlying decision-making process.
This primarily arises from the fact that post-hoc methods typically generate explanations using perturbation techniques, which create perturbed instances by altering the features of an instance, potentially pushing them outside the original data distribution.
Two approaches can address this issue.
One approach involving ensure that perturbed instances remain within the original data distribution, typically by leveraging generative models.
The other approach requires the model&rsquo;s predictions to remain unchanged when unimportant features in the instance are perturbed.
This paper adopts the latter approach."><meta name=author content><link rel=canonical href=https://lif323.github.io/posts/post-27/><link crossorigin=anonymous href=/assets/css/stylesheet.1d23bc8761bb8a516d01801663a24eeedbf97720019a7ddc0fc9ff29a2a8730d.css integrity="sha256-HSO8h2G7ilFtAYAWY6JO7tv5dyABmn3cD8n/KaKocw0=" rel="preload stylesheet" as=style><link rel=icon href=https://lif323.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://lif323.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://lif323.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://lif323.github.io/apple-touch-icon.png><link rel=mask-icon href=https://lif323.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://lif323.github.io/posts/post-27/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"\\[",right:"\\]",display:!0},{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})})</script><meta property="og:title" content="DiET"><meta property="og:description" content="A major issue with post-hoc explainability is its inability to faithfully represent the model&rsquo;s underlying decision-making process.
This primarily arises from the fact that post-hoc methods typically generate explanations using perturbation techniques, which create perturbed instances by altering the features of an instance, potentially pushing them outside the original data distribution.
Two approaches can address this issue.
One approach involving ensure that perturbed instances remain within the original data distribution, typically by leveraging generative models.
The other approach requires the model&rsquo;s predictions to remain unchanged when unimportant features in the instance are perturbed.
This paper adopts the latter approach."><meta property="og:type" content="article"><meta property="og:url" content="https://lif323.github.io/posts/post-27/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-09-14T21:29:31+08:00"><meta property="article:modified_time" content="2024-09-14T21:29:31+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="DiET"><meta name=twitter:description content="A major issue with post-hoc explainability is its inability to faithfully represent the model&rsquo;s underlying decision-making process.
This primarily arises from the fact that post-hoc methods typically generate explanations using perturbation techniques, which create perturbed instances by altering the features of an instance, potentially pushing them outside the original data distribution.
Two approaches can address this issue.
One approach involving ensure that perturbed instances remain within the original data distribution, typically by leveraging generative models.
The other approach requires the model&rsquo;s predictions to remain unchanged when unimportant features in the instance are perturbed.
This paper adopts the latter approach."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://lif323.github.io/posts/"},{"@type":"ListItem","position":2,"name":"DiET","item":"https://lif323.github.io/posts/post-27/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"DiET","name":"DiET","description":"A major issue with post-hoc explainability is its inability to faithfully represent the model\u0026rsquo;s underlying decision-making process. This primarily arises from the fact that post-hoc methods typically generate explanations using perturbation techniques, which create perturbed instances by altering the features of an instance, potentially pushing them outside the original data distribution.\nTwo approaches can address this issue. One approach involving ensure that perturbed instances remain within the original data distribution, typically by leveraging generative models. The other approach requires the model\u0026rsquo;s predictions to remain unchanged when unimportant features in the instance are perturbed. This paper adopts the latter approach.\n","keywords":[],"articleBody":"A major issue with post-hoc explainability is its inability to faithfully represent the model’s underlying decision-making process. This primarily arises from the fact that post-hoc methods typically generate explanations using perturbation techniques, which create perturbed instances by altering the features of an instance, potentially pushing them outside the original data distribution.\nTwo approaches can address this issue. One approach involving ensure that perturbed instances remain within the original data distribution, typically by leveraging generative models. The other approach requires the model’s predictions to remain unchanged when unimportant features in the instance are perturbed. This paper adopts the latter approach.\nThey propose the Distractor Erasure Tuning (DiET) method, which enables black-hox models to adapt to the removal of distractor, thereby enhancing model robustness. Consequently, DiET offters attributions that are both discriminative and faithful.\nThe following is the pseudocode of DiET. As shown in the pseudocode, DiET employs two objectives that alternately minimized between $\\theta$ and $m$.\nDiET optimizes $\\mathcal{L}_{QFA}$ to obtain the optimal mask. Notably, the mask in this paper defines the signal-distractor decomposition. Assuming that $f_v$ is a $\\mathcal{Q}$-robust model, $f_v$ exhibits robustness to the counterfactual distribution $\\mathcal{Q}$. Specifically, the feature removal process is governed by the counterfactual distribution $\\mathcal{Q}$. $$ \\mathcal{L}_{\\mathrm{QFA}}\\left(\\theta,\\{\\mathbf{m}(\\mathbf{x})\\}_{\\mathbf{x} \\in \\mathcal{X}}\\right)=\\underset{\\mathbf{x} \\in \\mathcal{X}}{\\mathbb{E}}[\\underbrace{\\|\\mathbf{m}(\\mathbf{x})\\|_1}_{\\text {mask sparsity }}+\\lambda_1 \\underbrace{\\left.\\| f_v(\\mathbf{x} ; \\theta)-f_v\\left(\\mathbf{x}_{\\mathbf{s}}(\\mathbf{m}, q)\\right) ; \\theta\\right) \\|_1}_{\\text {data distillation }}] $$ The objective function is similar to other mask-based methods. The key difference is that DiET utilizes a $\\mathcal{Q}$-robust model instead of the original model.\nAnother objective is $\\mathcal{L}_{\\text {train }}$. Assuming the optimal signal-distractor decomposition $m$ is known, this objective aims to obtain $f_v$ by optimizing $\\mathcal{L}_{\\text {train }}$. $$\\mathcal{L}_{\\text {train }}\\left(\\theta,\\{\\mathbf{m}(\\mathbf{x})\\}_{\\mathbf{x} \\in \\mathcal{X}}\\right)=\\underset{\\mathbf{x} \\in \\mathcal{X}}{\\mathbb{E}}[\\underbrace{\\left.\\| f_v(\\mathbf{x} ; \\theta)-f_v\\left(\\mathbf{x}_{\\mathbf{s}}(\\mathbf{m}(\\mathbf{x}), q)\\right) ; \\theta\\right) \\|_1}_{\\text {data distillation }}+\\lambda_2 \\underbrace{\\left\\|f_b(\\mathbf{x})-f_v(\\mathbf{x} ; \\theta)\\right\\|_1}_{\\text {model distillation }}] $$References Bhalla, U., Srinivas, S., \u0026 Lakkaraju, H. (2024). Discriminative feature attributions: bridging post hoc explainability and inherent interpretability. Advances in Neural Information Processing Systems, 36.\n","wordCount":"319","inLanguage":"en","datePublished":"2024-09-14T21:29:31+08:00","dateModified":"2024-09-14T21:29:31+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://lif323.github.io/posts/post-27/"},"publisher":{"@type":"Organization","name":"lif323","logo":{"@type":"ImageObject","url":"https://lif323.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://lif323.github.io/ accesskey=h title="lif323 (Alt + H)">lif323</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">DiET</h1><div class=post-meta><span title='2024-09-14 21:29:31 +0800 +0800'>September 14, 2024</span></div></header><div class=post-content><p>A major issue with post-hoc explainability is its inability to faithfully represent the model&rsquo;s underlying decision-making process.
This primarily arises from the fact that post-hoc methods typically generate explanations using perturbation techniques, which create perturbed instances by altering the features of an instance, potentially pushing them outside the original data distribution.</p><p>Two approaches can address this issue.
One approach involving ensure that perturbed instances remain within the original data distribution, typically by leveraging generative models.
The other approach requires the model&rsquo;s predictions to remain unchanged when unimportant features in the instance are perturbed.
This paper adopts the latter approach.</p><p>They propose the Distractor Erasure Tuning (DiET) method, which enables black-hox models to adapt to the removal of distractor, thereby enhancing model robustness. Consequently, DiET offters attributions that are both discriminative and faithful.</p><p>The following is the pseudocode of DiET.
<img loading=lazy src=image.png alt="alt text"></p><p>As shown in the pseudocode, DiET employs two objectives that alternately minimized between $\theta$ and $m$.</p><p>DiET optimizes $\mathcal{L}_{QFA}$ to obtain the optimal mask. Notably, the mask in this paper defines the signal-distractor decomposition.
Assuming that $f_v$ is a $\mathcal{Q}$-robust model, $f_v$ exhibits robustness to the counterfactual distribution $\mathcal{Q}$. Specifically, the feature removal process is governed by the counterfactual distribution $\mathcal{Q}$.</p>$$
\mathcal{L}_{\mathrm{QFA}}\left(\theta,\{\mathbf{m}(\mathbf{x})\}_{\mathbf{x} \in \mathcal{X}}\right)=\underset{\mathbf{x} \in \mathcal{X}}{\mathbb{E}}[\underbrace{\|\mathbf{m}(\mathbf{x})\|_1}_{\text {mask sparsity }}+\lambda_1 \underbrace{\left.\| f_v(\mathbf{x} ; \theta)-f_v\left(\mathbf{x}_{\mathbf{s}}(\mathbf{m}, q)\right) ; \theta\right) \|_1}_{\text {data distillation }}]
$$<p>The objective function is similar to other mask-based methods. The key difference is that DiET utilizes a $\mathcal{Q}$-robust model instead of the original model.</p><p>Another objective is $\mathcal{L}_{\text {train }}$. Assuming the optimal signal-distractor decomposition $m$ is known, this objective aims to obtain $f_v$ by optimizing $\mathcal{L}_{\text {train }}$.</p>$$\mathcal{L}_{\text {train }}\left(\theta,\{\mathbf{m}(\mathbf{x})\}_{\mathbf{x} \in \mathcal{X}}\right)=\underset{\mathbf{x} \in \mathcal{X}}{\mathbb{E}}[\underbrace{\left.\| f_v(\mathbf{x} ; \theta)-f_v\left(\mathbf{x}_{\mathbf{s}}(\mathbf{m}(\mathbf{x}), q)\right) ; \theta\right) \|_1}_{\text {data distillation }}+\lambda_2 \underbrace{\left\|f_b(\mathbf{x})-f_v(\mathbf{x} ; \theta)\right\|_1}_{\text {model distillation }}]
$$<h1 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h1><p>Bhalla, U., Srinivas, S., & Lakkaraju, H. (2024). Discriminative feature attributions: bridging post hoc explainability and inherent interpretability. Advances in Neural Information Processing Systems, 36.</p></div><footer class=post-footer><ul class=post-tags></ul></footer><script src=https://giscus.app/client.js data-repo=lif323/lif323.github.io data-repo-id=R_kgDOLtjKFA data-category=Announcements data-category-id=DIC_kwDOLtjKFM4Ce08J data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=light data-lang=en crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2024 <a href=https://lif323.github.io/>lif323</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>