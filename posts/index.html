<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Posts | lif323</title>
<meta name=keywords content><meta name=description content="Posts - lif323"><meta name=author content><link rel=canonical href=https://lif323.github.io/posts/><link crossorigin=anonymous href=/assets/css/stylesheet.1d23bc8761bb8a516d01801663a24eeedbf97720019a7ddc0fc9ff29a2a8730d.css integrity="sha256-HSO8h2G7ilFtAYAWY6JO7tv5dyABmn3cD8n/KaKocw0=" rel="preload stylesheet" as=style><link rel=icon href=https://lif323.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://lif323.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://lif323.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://lif323.github.io/apple-touch-icon.png><link rel=mask-icon href=https://lif323.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://lif323.github.io/posts/index.xml><link rel=alternate hreflang=en href=https://lif323.github.io/posts/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"\\[",right:"\\]",display:!0},{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})})</script><meta property="og:title" content="Posts"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="https://lif323.github.io/posts/"><meta name=twitter:card content="summary"><meta name=twitter:title content="Posts"><meta name=twitter:description content><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://lif323.github.io/posts/"}]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://lif323.github.io/ accesskey=h title="lif323 (Alt + H)">lif323</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><header class=page-header><h1>Posts</h1></header><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Note on Out-of-Distribution Problem and Search Methods for Feature Importance</h2></header><div class=entry-content><p>The main contributions of this paper are twofold:(1) investigating the Out-of-Distribution (OOD) problem in counterfactual inputs, and (2) proposing a Parallel Local Search (PLS) method for generating explanations.
Out-of-Distribution Problem The possible causes of the OOD problem in FI explanations are shown in the following figure. Even on in-distribution data, neural networks are sensitive to random parameter initialization, data ordering, and hyperparameters. Therefore, neural networks are also influenced by these factors when processing OOD data.
...</p></div><footer class=entry-footer><span title='2024-12-19 16:31:26 +0800 +0800'>December 19, 2024</span></footer><a class=entry-link aria-label="post link to Note on Out-of-Distribution Problem and Search Methods for Feature Importance" href=https://lif323.github.io/posts/post-41/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Note on What You See Is What You Classify</h2></header><div class=entry-content><p>$$ \begin{equation} \begin{aligned} \mathbf{q}_{\text{mask}} &= \operatorname{sort}(\operatorname{vector}(\mathbf{s})) \in [0, 1]^Z \\ \mathbf{q}_{\text{min}} &= [\mathbf{1}^{\lfloor Za\rfloor}, \cdots, \mathbf{0}^{\lfloor 1 - Za\rfloor}] \\ \mathbf{q}_{\text{max}} &= [\mathbf{1}^{\lfloor Zb \rfloor}, \cdots, \mathbf{0}^{\lfloor 1 - Zb \rfloor}] \\ \end{aligned} \end{equation} $$$$ \mathcal{B}(\mathbf{s}) = \frac{1}{Z}\sum_{i=1}^{Z}\max(\mathbf{q}_{\text{min}}[i] - \mathbf{q}_{\text{mask}}[i], 0) + \frac{1}{Z}\sum_{i=1}^{Z}\max(\mathbf{q}_{\text{mask}}[i] - \mathbf{q}_{\text{max}}[i], 0). $$ where the first term forces the attribution $\mathbf{q}_{\text{mask}}[:\lfloor Za\rfloor]$ to approach 1, while the second term drives $\mathbf{q}_{\text{mask}}[\lfloor Z - Zb\rfloor:]$ to approach 0. Consequently, the number of non-zero attribution, N, satisfies $Za \leq N \leq Zb$.
...</p></div><footer class=entry-footer><span title='2024-12-16 21:56:05 +0800 +0800'>December 16, 2024</span></footer><a class=entry-link aria-label="post link to Note on What You See Is What You Classify" href=https://lif323.github.io/posts/post-40/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Selective Explanations NeurIPS 2024</h2></header><div class=entry-content><p>When explaining large black-box model with a large number of parameters, this paper classify existing feature attribution methods into three groups:
(1) Original feature attribution methods. They usually be prohibitively expensive for the large models due to each individual explanations requires a significant number of inferences.
(2) Monte Carlo methods. They employ Monte Carlo methods to approximate explanations with fewer computations.
(3) Amortized methods. They train a separate model to mimic the output of original feature attribution methods.
...</p></div><footer class=entry-footer><span title='2024-10-14 16:26:32 +0800 +0800'>October 14, 2024</span></footer><a class=entry-link aria-label="post link to Selective Explanations NeurIPS 2024" href=https://lif323.github.io/posts/post-35/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>CoRTX ICLR 2023</h2></header><div class=entry-content><p>Before introducing this work, I will first present the Real-time explainer (RTX) framework. RTX is a one-feed-forward explainer that can generate model explanations more efficiently. A major limitation of existing RTX approaches is their reliance on a large number of explanation labels. In my view, RTX is a neural network that generates explanations but like other domains, it also requires substantial data for training.
However, due to limited computational resources and constrained human efforts. accurate explanation labels are difficult to obtain. To address this issue, thie paper proposes the Contrastive Real-Time eXplanation (CoRTX) method. CoRTX trains an encoder using contrastive learning to learn latent explanations in a self-supervised manner, thereby allevating the challenge of data scarcity.
...</p></div><footer class=entry-footer><span title='2024-09-25 09:54:31 +0800 +0800'>September 25, 2024</span></footer><a class=entry-link aria-label="post link to CoRTX ICLR 2023" href=https://lif323.github.io/posts/post-31/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>VeriX (Verified eXplainability) NeurIPS 2023</h2></header><div class=entry-content><p>This paper presents VeriX, a tool for generating optimal robust explanations and counterfactuals along decision boundaries of deep neural networks.
Specifically, its perturbations are bounded.
The pseudocode is shown below.
Overall, VeriX iterates through each feature in $\mathbf{x}$, checking whether the feature satisfies the following condition: a bounded perturbation on $\mathbf{B} \cup \{i\}$ dose not alter the modelâ€™s prediction. If the condition is met, the feature is considered unimportant; otherwise, it is deemed important. In my view, while VeriX successfully identifies important feature, it does not appear to provide an importance score for each feature.
...</p></div><footer class=entry-footer><span title='2024-09-19 16:35:29 +0800 +0800'>September 19, 2024</span></footer><a class=entry-link aria-label="post link to VeriX (Verified eXplainability) NeurIPS 2023" href=https://lif323.github.io/posts/post-30/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>MAE: Masked Autoencoder CVPR 2021</h2></header><div class=entry-content><p>This paper proposes a scalable self-supervised learning method-Masked Autoencoders (MAE) for computer vision. MAE reconstructs the missing pixels from randomly masked images. MAE has two key design elements.
Firstly, the encoder-decoder architecture is asymmetric. The encoder processes only the visible parts of the image, ignoring the masked pixels. However, the decoderâ€™s input includes not only the encoded visiable pixels but also the masked tokens. Specifically, the masked tokens are shared, learnable vectors that represent the missing pixels to predicted. Notably, the decoder is lightweight.
...</p></div><footer class=entry-footer><span title='2024-09-18 10:38:12 +0800 +0800'>September 18, 2024</span></footer><a class=entry-link aria-label="post link to MAE: Masked Autoencoder CVPR 2021" href=https://lif323.github.io/posts/post-29/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Moco CVPR 2020</h2></header><div class=entry-content><p>$$ \theta_{k} \leftarrow m \theta_{k} + (1 - m)\theta_{q}, $$ where $\theta_k$ represents the parameters of the momentum encoder and $\theta_q$ represents the parameters of the encoder. The update to the momentum encoder should be gradual. Experimental results show that rapidly changing encoders lead to suboptimal results. They hypothesize that this is because rapidly changing encoders reduce the consistency of key representations. Therefore, in the experiments, they set m between 0.99 and 0.9999.
...</p></div><footer class=entry-footer><span title='2024-09-16 17:03:35 +0800 +0800'>September 16, 2024</span></footer><a class=entry-link aria-label="post link to Moco CVPR 2020" href=https://lif323.github.io/posts/post-28/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>DiET NeurIPS 2023</h2></header><div class=entry-content><p>A major issue with post-hoc explainability is its inability to faithfully represent the modelâ€™s underlying decision-making process. This primarily arises from the fact that post-hoc methods typically generate explanations using perturbation techniques, which create perturbed instances by altering the features of an instance, potentially pushing them outside the original data distribution.
Two approaches can address this issue. One approach involving ensure that perturbed instances remain within the original data distribution, typically by leveraging generative models. The other approach requires the modelâ€™s predictions to remain unchanged when unimportant features in the instance are perturbed. This paper adopts the latter approach.
...</p></div><footer class=entry-footer><span title='2024-09-14 21:29:31 +0800 +0800'>September 14, 2024</span></footer><a class=entry-link aria-label="post link to DiET NeurIPS 2023" href=https://lif323.github.io/posts/post-27/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>RISE: Randomized Input Sampling for Explanation BMVC 2018</h2></header><div class=entry-content><p>The key idea of RISE to measure the importance of an image region is to obscure or â€˜perturbâ€™ it and observe how much this affects the black box decision.
RISE $$ S_{\mathbf{I}, f}(\lambda) = \mathbb{E}_{\mathbf{M}}\left[f(\mathbf{I}\odot \mathbf{M}) | \mathbf{M}(\lambda) = 1\right] $$. The intuition behind this is that $f(I \cdot M)$ is high when pixels preserved by mask M are important.
$$ \begin{aligned} S_{\mathbf{I}, f}(\lambda) &= \mathbb{E}_{\mathbf{M}}\left[f(\mathbf{I}\odot \mathbf{M}) | \mathbf{M}(\lambda) = 1\right]\\ &= \sum_{m}f(\mathbf{I}\odot m) P[\mathbf{M} = m | \mathbf{M}(\lambda) = 1]\\ &= \frac{1}{P[\mathbf{M}(\lambda) = 1]} \sum_{m}f(\mathbf{I}\odot m) P[\mathbf{M} = m, \mathbf{M}(\lambda)=1] \end{aligned} $$$$ \begin{aligned} P[\mathbf{M} = m, \mathbf{M}(\lambda)=1] &= \begin{cases} 0, & \text{if } m(\lambda) =0, \\ P[\mathbf{M} = m], & \text{if } m(\lambda) = 1, \end{cases}\\ & = m(\lambda)P[\mathbf{M} = m] \end{aligned} $$$$ S_{\mathbf{I}, f}(\lambda) = \frac{1}{P[\mathbf{M}(\lambda) = 1]} \sum_{m}f(\mathbf{I}\odot m) m(\lambda)P[\mathbf{M} = m] $$ where, $P[\mathbf{M}(\lambda) = 1] = \mathbb{E}(\mathbf{M}[\lambda)]$.
...</p></div><footer class=entry-footer><span title='2024-09-05 22:34:19 +0800 +0800'>September 5, 2024</span></footer><a class=entry-link aria-label="post link to RISE: Randomized Input Sampling for Explanation BMVC 2018" href=https://lif323.github.io/posts/post-26/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Frequency masking arXiv 2024</h2></header><div class=entry-content><p>This paper proposes a method called FreqRISE. Current apporaches assume that salient information resides in the time domain (the raw input space), they argue that this assumption is less reasonable and that salient information is more likely to reside in the frequency domain.
$$ \hat{\mathbf{X}} = \mathbf{X} \odot \mathbf{M}. $$$$ \hat{\mathbf{y}} = f(\hat{\mathbf{X}}) $$In this work, they apply a mask to a different domain (frequency domain) rather than the time domain. They assume the existence of an invertible mapping to the domain of interest (the frequency domain), $g: \mathbf{X}^T \rightarrow \mathbf{X}^S$.
...</p></div><footer class=entry-footer><span title='2024-09-05 16:37:03 +0800 +0800'>September 5, 2024</span></footer><a class=entry-link aria-label="post link to Frequency masking arXiv 2024" href=https://lif323.github.io/posts/post-25/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://lif323.github.io/posts/page/2/>Next&nbsp;&nbsp;Â»</a></nav></footer></main><footer class=footer><span>&copy; 2024 <a href=https://lif323.github.io/>lif323</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>