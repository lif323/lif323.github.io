<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Posts on lif323</title><link>https://lif323.github.io/posts/</link><description>Recent content in Posts on lif323</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Mon, 23 Dec 2024 22:52:24 +0800</lastBuildDate><atom:link href="https://lif323.github.io/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>NeurIPS 2023 Robustness of Removal-Based Feature Attributions</title><link>https://lif323.github.io/posts/post-42/</link><pubDate>Mon, 23 Dec 2024 22:52:24 +0800</pubDate><guid>https://lif323.github.io/posts/post-42/</guid><description>&lt;p>This paper provides theoretical guarantees for the robustness of model predictions to input and model perturbations in removal-based feature attribution methods. It then validates the theoretical results using synthetic and real-world datasets.&lt;/p>
&lt;p>Notably, it describes removal-based feature attribution in two parts:(1) &lt;strong>how feature information is removed from the model&lt;/strong>, and (2) &lt;strong>how the algorithm summarizes each feature&amp;rsquo;s influence&lt;/strong>.&lt;/p>
&lt;p>The main theory they apply is that L-Lipschitz continuous property: if a model $f$ is globally L-Lipschitz continuous, we have&lt;/p></description></item><item><title>Note on Out-of-Distribution Problem and Search Methods for Feature Importance</title><link>https://lif323.github.io/posts/post-41/</link><pubDate>Thu, 19 Dec 2024 16:31:26 +0800</pubDate><guid>https://lif323.github.io/posts/post-41/</guid><description>&lt;p>The main contributions of this paper are twofold:(1) investigating the Out-of-Distribution (OOD) problem in counterfactual inputs, and (2) proposing a Parallel Local Search (PLS) method for generating explanations.&lt;/p>
&lt;h1 id="out-of-distribution-problem">Out-of-Distribution Problem&lt;/h1>
&lt;p>The possible causes of the OOD problem in FI explanations are shown in the following figure.
&lt;img loading="lazy" src="image.png" alt="alt text" />
Even on in-distribution data, neural networks are sensitive to random parameter initialization, data ordering, and hyperparameters.
Therefore, neural networks are also influenced by these factors when processing OOD data.&lt;/p></description></item><item><title>Selective Explanations NeurIPS 2024</title><link>https://lif323.github.io/posts/post-35/</link><pubDate>Mon, 14 Oct 2024 16:26:32 +0800</pubDate><guid>https://lif323.github.io/posts/post-35/</guid><description>&lt;p>When explaining large black-box model with a large number of parameters, this paper classify existing feature attribution methods into three groups:&lt;/p>
&lt;p>(1) Original feature attribution methods. They usually be prohibitively expensive for the large models due to each individual explanations requires a significant number of inferences.&lt;/p>
&lt;p>(2) Monte Carlo methods. They employ Monte Carlo methods to approximate explanations with fewer computations.&lt;/p>
&lt;p>(3) Amortized methods. They train a separate model to mimic the output of original feature attribution methods.&lt;/p></description></item><item><title>CoRTX ICLR 2023</title><link>https://lif323.github.io/posts/post-31/</link><pubDate>Wed, 25 Sep 2024 09:54:31 +0800</pubDate><guid>https://lif323.github.io/posts/post-31/</guid><description>&lt;p>Before introducing this work, I will first present the Real-time explainer (RTX) framework. RTX is a one-feed-forward explainer that can generate model explanations more efficiently. A major limitation of existing RTX approaches is their reliance on a large number of explanation labels. In my view, RTX is a neural network that generates explanations but like other domains, it also requires substantial data for training.&lt;/p>
&lt;p>However, due to limited computational resources and constrained human efforts. accurate explanation labels are difficult to obtain. To address this issue, thie paper proposes the Contrastive Real-Time eXplanation (CoRTX) method. CoRTX trains an encoder using contrastive learning to learn latent explanations in a self-supervised manner, thereby allevating the challenge of data scarcity.&lt;/p></description></item><item><title>VeriX (Verified eXplainability) NeurIPS 2023</title><link>https://lif323.github.io/posts/post-30/</link><pubDate>Thu, 19 Sep 2024 16:35:29 +0800</pubDate><guid>https://lif323.github.io/posts/post-30/</guid><description>&lt;p>This paper presents VeriX, a tool for generating optimal robust explanations and counterfactuals along decision boundaries of deep neural networks.&lt;/p>
&lt;p>Specifically, its perturbations are bounded.&lt;/p>
&lt;p>The pseudocode is shown below.&lt;/p>
&lt;p>&lt;img loading="lazy" src="image.png" alt="alt text" />
&lt;/p>
&lt;p>Overall, VeriX iterates through each feature in $\mathbf{x}$, checking whether the feature satisfies the following condition: a bounded perturbation on $\mathbf{B} \cup \{i\}$ dose not alter the model&amp;rsquo;s prediction. If the condition is met, the feature is considered unimportant; otherwise, it is deemed important. In my view, while VeriX successfully identifies important feature, it does not appear to provide an importance score for each feature.&lt;/p></description></item><item><title>MAE: Masked Autoencoder CVPR 2021</title><link>https://lif323.github.io/posts/post-29/</link><pubDate>Wed, 18 Sep 2024 10:38:12 +0800</pubDate><guid>https://lif323.github.io/posts/post-29/</guid><description>&lt;p>This paper proposes a scalable self-supervised learning method-Masked Autoencoders (MAE) for computer vision. MAE reconstructs the missing pixels from randomly masked images. MAE has two key design elements.&lt;/p>
&lt;p>Firstly, the encoder-decoder architecture is asymmetric. The encoder processes only the visible parts of the image, ignoring the masked pixels.
However, the decoder&amp;rsquo;s input includes not only the encoded visiable pixels but also the masked tokens. Specifically, the masked tokens are shared, learnable vectors that represent the missing pixels to predicted. Notably, the decoder is lightweight.&lt;/p></description></item><item><title>Moco CVPR 2020</title><link>https://lif323.github.io/posts/post-28/</link><pubDate>Mon, 16 Sep 2024 17:03:35 +0800</pubDate><guid>https://lif323.github.io/posts/post-28/</guid><description>$$
\theta_{k} \leftarrow m \theta_{k} + (1 - m)\theta_{q},
$$&lt;p>
where $\theta_k$ represents the parameters of the momentum encoder and $\theta_q$ represents the parameters of the encoder. The update to the momentum encoder should be gradual. Experimental results show that rapidly changing encoders lead to suboptimal results. They hypothesize that this is because rapidly changing encoders reduce the consistency of key representations. Therefore, in the experiments, they set m between 0.99 and 0.9999.&lt;/p></description></item><item><title>DiET NeurIPS 2023</title><link>https://lif323.github.io/posts/post-27/</link><pubDate>Sat, 14 Sep 2024 21:29:31 +0800</pubDate><guid>https://lif323.github.io/posts/post-27/</guid><description>&lt;p>A major issue with post-hoc explainability is its inability to faithfully represent the model&amp;rsquo;s underlying decision-making process.
This primarily arises from the fact that post-hoc methods typically generate explanations using perturbation techniques, which create perturbed instances by altering the features of an instance, potentially pushing them outside the original data distribution.&lt;/p>
&lt;p>Two approaches can address this issue.
One approach involving ensure that perturbed instances remain within the original data distribution, typically by leveraging generative models.
The other approach requires the model&amp;rsquo;s predictions to remain unchanged when unimportant features in the instance are perturbed.
This paper adopts the latter approach.&lt;/p></description></item><item><title>RISE: Randomized Input Sampling for Explanation BMVC 2018</title><link>https://lif323.github.io/posts/post-26/</link><pubDate>Thu, 05 Sep 2024 22:34:19 +0800</pubDate><guid>https://lif323.github.io/posts/post-26/</guid><description>&lt;p>The key idea of RISE to measure the importance of an image region is to obscure or &amp;lsquo;perturb&amp;rsquo; it and observe how much this affects the black box decision.&lt;/p>
&lt;h1 id="rise">RISE&lt;/h1>
$$
S_{\mathbf{I}, f}(\lambda) = \mathbb{E}_{\mathbf{M}}\left[f(\mathbf{I}\odot \mathbf{M}) | \mathbf{M}(\lambda) = 1\right]
$$&lt;p>.
The intuition behind this is that $f(I \cdot M)$ is high when pixels preserved by mask M are important.&lt;/p>
$$
\begin{aligned}
S_{\mathbf{I}, f}(\lambda) &amp;= \mathbb{E}_{\mathbf{M}}\left[f(\mathbf{I}\odot \mathbf{M}) | \mathbf{M}(\lambda) = 1\right]\\
&amp;= \sum_{m}f(\mathbf{I}\odot m) P[\mathbf{M} = m | \mathbf{M}(\lambda) = 1]\\
&amp;= \frac{1}{P[\mathbf{M}(\lambda) = 1]} \sum_{m}f(\mathbf{I}\odot m) P[\mathbf{M} = m, \mathbf{M}(\lambda)=1]
\end{aligned}
$$$$
\begin{aligned}
P[\mathbf{M} = m, \mathbf{M}(\lambda)=1] &amp;= \begin{cases}
0, &amp; \text{if } m(\lambda) =0, \\
P[\mathbf{M} = m], &amp; \text{if } m(\lambda) = 1,
\end{cases}\\
&amp; = m(\lambda)P[\mathbf{M} = m]
\end{aligned}
$$$$
S_{\mathbf{I}, f}(\lambda) = \frac{1}{P[\mathbf{M}(\lambda) = 1]} \sum_{m}f(\mathbf{I}\odot m) m(\lambda)P[\mathbf{M} = m]
$$&lt;p>
where, $P[\mathbf{M}(\lambda) = 1] = \mathbb{E}(\mathbf{M}[\lambda)]$.&lt;/p></description></item><item><title>Frequency masking arXiv 2024</title><link>https://lif323.github.io/posts/post-25/</link><pubDate>Thu, 05 Sep 2024 16:37:03 +0800</pubDate><guid>https://lif323.github.io/posts/post-25/</guid><description>&lt;p>This paper proposes a method called FreqRISE. Current apporaches assume that salient information resides in the time domain (the raw input space), they argue that this assumption is less reasonable and that salient information is more likely to reside in the frequency domain.&lt;/p>
$$
\hat{\mathbf{X}} = \mathbf{X} \odot \mathbf{M}.
$$$$
\hat{\mathbf{y}} = f(\hat{\mathbf{X}})
$$&lt;p>In this work, they apply a mask to a different domain (frequency domain) rather than the time domain.
They assume the existence of an invertible mapping to the domain of interest (the frequency domain), $g: \mathbf{X}^T \rightarrow \mathbf{X}^S$.&lt;/p></description></item><item><title>Maximum Likelihood Estimation and KL Divergence</title><link>https://lif323.github.io/posts/post-24/</link><pubDate>Wed, 04 Sep 2024 10:43:33 +0800</pubDate><guid>https://lif323.github.io/posts/post-24/</guid><description>&lt;p>The core idea of maximum likelihood estimation is to assume that we draw samples $\{x^1, x^2, \cdots, x^m\}$ from the data distribution $P_{\text{data}}(x)$ and calculate the probability $P_{\theta}(x^i)$ of observing each sample $x^i$. The objective is to find the parameters $\theta$ that maximize the likelihood of observing all the samples. The optimization objective is formulated as follows:&lt;/p>
$$
\theta^{*} = \argmax_{\theta} \prod_{i=1}^{m} P_{\theta}(x^i)
$$$$
\begin{aligned}
\theta^{*} &amp;= \argmax_{\theta} \prod_{i=1}^{m} P_{\theta}(x^i)\\
&amp; = \argmax_{\theta} \log \prod_{i=1}^{m} P_{\theta}(x^i)\\
&amp; = \argmax_{\theta} \sum_{i=1}^{m} \log P_{\theta}(x^i)\\
&amp; \approx \argmax_{\theta} E_{x \sim P_{\text{data}}} \left[ \log P_{\theta}(x^i) \right]\\
&amp; = \argmax_{\theta} \int_{x} P_{\text{data}}(x) \log P_{\theta}(x) dx\\
&amp; = \argmax_{\theta} \int_{x} P_{\text{data}}(x) \log P_{\theta}(x) dx - \underbrace{\int_{x} P_{\text{data}}(x) \log P_{\text{data}}(x) dx}_{\text{not related to $\theta$}}\\
&amp; = \argmax_{\theta} \int_{x} P_{\text{data}}(x) \log \frac{P_{\theta}(x)} {P_{\text{data}}(x)} dx\\
&amp; = \argmax_{\theta} -1 \cdot \int_{x} P_{\text{data}}(x) \log \frac{P_{\text{data}}(x)} {P_{\theta}(x)} dx\\
&amp; = \argmin_{\theta} KL(P_{\text{data}}|| P_{\theta})
\end{aligned}
$$&lt;h1 id="references">References&lt;/h1>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=67_M2qP5ssY&amp;amp;list=PLJV_el3uVTsNi7PgekEUFsyVllAJXRsP-">https://www.youtube.com/watch?v=67_M2qP5ssY&amp;amp;list=PLJV_el3uVTsNi7PgekEUFsyVllAJXRsP-&lt;/a>&lt;/p></description></item><item><title>CEM NeurIPS 2018</title><link>https://lif323.github.io/posts/post-20/</link><pubDate>Mon, 26 Aug 2024 10:44:24 +0800</pubDate><guid>https://lif323.github.io/posts/post-20/</guid><description>&lt;p>This paper proposes a Contrastive Explanations method(CEM).
They define a type of explanation where, given an input, the goal is to identify which features are both minimal and sufficient to justify its classification as well as which features are minimal and necessarily absent.&lt;/p>
&lt;p>&lt;strong>Pertinent Negatives (PN)&lt;/strong> refers to the importance of missing features in model predictions. It essentially represents a counterfactual explanation. $\mathbf{x}_0 + \boldsymbol{\delta}$ denotes the counterfactual instance. A pertinent negative can be identified by solving the following optimization problem:&lt;/p></description></item><item><title>Note Integrated Directional Gradients</title><link>https://lif323.github.io/posts/post-17/note-integrated-directional-gradients/</link><pubDate>Mon, 05 Aug 2024 16:07:54 +0800</pubDate><guid>https://lif323.github.io/posts/post-17/note-integrated-directional-gradients/</guid><description>&lt;p>This paper proposes the Integrated Directional Gradients (IDG) to computing feature group attribution. One important thing is to find the family of meaningful feature subsets which is defined by the domain related methods, such as the constituency parse tree for NLP.&lt;/p>
$$
z_i^S =
\begin{cases}
x_i - b_i &amp; \text {if} a_i \in S\\
0 &amp; \text{ otherwise}
\end{cases}
$$$$
\nabla_{S}f(x) = \nabla f(x) \cdot \hat{z}^{S} \quad \hat{z}^{S} = \frac{z^{S}}{\Vert z^{S} \Vert}
$$$$
\text{IDG}(S) = \int_{\alpha = 0}^1 \nabla_{S}f(b + \alpha (x - b)) d \alpha
$$&lt;p>
The dividend $d(S)$ of the feature subset $S$ is computed by normalizing the absolute value of $IDG(S)$ over all meaningful subsets.&lt;/p></description></item><item><title>Note Ranking Problems</title><link>https://lif323.github.io/posts/post-16/</link><pubDate>Sun, 14 Jul 2024 09:09:56 +0800</pubDate><guid>https://lif323.github.io/posts/post-16/</guid><description>&lt;p>In machine learning, most problems are classified as Classification or Regression. However, this categorization is not always suitable for real-world problems. In some tasks, the classes are imbalanced but exhibit some weak order. In these cases, the loss functions for Classification and Regression do not perform well. This blog will apply the less commonly used margin ranking loss to address this issue.&lt;/p>
&lt;p>Here is an example from the UCI Wine Quality Dataset from Cortez et al. Suppose we are a wine supplier and we find that a high review in some wine magazines increases the demand for a particular wine. Therefore, we want to predict the review score based on some measurable features of the wine. This way, we can increase the stock of the corresponding wine in advance, before the demand rises.&lt;/p></description></item><item><title>Note Discretized Integrated Gradients EMNLP 2021</title><link>https://lif323.github.io/posts/post-15/</link><pubDate>Mon, 08 Jul 2024 15:58:29 +0800</pubDate><guid>https://lif323.github.io/posts/post-15/</guid><description>&lt;p>This paper proposes the Discretized Integrated Gradients (DIG) method, which effectively applies the Integrated Gradients(IG) method to the word embedding space.&lt;/p>
&lt;p>Integrated Gradient (IG) measures the importance of features by using the average of model gradients at interpolation points along a straight path in the input space.&lt;/p>
&lt;p>However, in the word embedding space, due to its discrete nature, interpolation points may not accurately represent textual data. In particular cases, interpolation points could be outliers.&lt;/p></description></item><item><title>Note Learning Perturbations ICML 2023</title><link>https://lif323.github.io/posts/post-14/note-learningperturbations-2023/</link><pubDate>Sun, 07 Jul 2024 15:00:05 +0800</pubDate><guid>https://lif323.github.io/posts/post-14/note-learningperturbations-2023/</guid><description>&lt;p>This paper improves existing methods that provide explanations using trainable masks.
Inspired by methods for static data, the current approach uses fixed perturbations. However, this paper argues that this approach may not be suitable for time series data.
In this paper, their method not only has a trainable mask but also trainable perturbations.&lt;/p>
$$
\Phi(\mathbf{x}, \mathbf{m}) = \mathbf{m} \times \mathbf{x} + (1 - \mathbf{m}) \times g(\mathbf{x}),
$$&lt;p>
where $g(\mathbf{x})$ is a function of the input. A possible definition is $g(\mathbf{x}) = \frac{1}{W}\sum_{t'=t-W}^{t}x_{t'}$.&lt;/p></description></item><item><title>KL Divergence and Cross Entropy</title><link>https://lif323.github.io/posts/post-12/kl-divergence-and-cross-entropy/</link><pubDate>Mon, 24 Jun 2024 21:37:58 +0800</pubDate><guid>https://lif323.github.io/posts/post-12/kl-divergence-and-cross-entropy/</guid><description>$$
H(P, Q) = - \sum_{x}p(x)\log Q(x)
$$$$
KL(P|Q) = \sum_{x}P(x)\log\frac{P(x)}{Q(x)}
$$$$
S(v) = - \sum_{i}p(v_i)\log p(v_i),
$$&lt;p>
where $p(v_i)$ represents the probability of state $v_i$. From the perspective of information theory, $S(v)$ is the information required to remove system uncertainty.&lt;/p>
$$
KL(A|B) = \sum_{i}p_A(v_i)\log p_A(v_i) - p_A(v_i)\log p_B(v_i),
$$&lt;p>
where the first term of the right hand side is the entropy of distribution $A$, the second term can be interpreted as the expectation of distribution $B$ in terms of $A$. The $KL(A|B)$ describes how different $B$ is from $A$ from the perspective of $A$.&lt;/p></description></item><item><title>Note on RealTimeImageSaliency NeurIPS 2017</title><link>https://lif323.github.io/posts/post-11/note-on-realtimeimagesaliency-neurips-2017/</link><pubDate>Mon, 24 Jun 2024 15:59:28 +0800</pubDate><guid>https://lif323.github.io/posts/post-11/note-on-realtimeimagesaliency-neurips-2017/</guid><description>$$
\Phi(X, M) = X \odot M + A \odot (1 - M),
$$&lt;p>
where $X$ is the original input, and $A$ is a reference input, which is usually a highly blurred version of $X$.&lt;/p>
$$
TV(M) = \sum_{i,j}(M_{ij} - M_{ij+1})^2 + \sum_{i,j}(M_{ij} - M_{i+1j})^2
$$$$
L(M) = \lambda_1 TV(M) + \lambda_2 AV(M) - \log (f_{c}(\Phi(X, M))) + \lambda_3 f_c(\Phi(X, 1- M))^{\lambda_4}
$$&lt;p>
The third term makes sure that the classifier is able to recognize the selected class from the preserved region.
It is worth noting that the last term ensures that the probability of the selected class after the salient region is removed, is low. Setting $\lambda_4$ to a value smaller than 1 helps reduce this probability to very small values.&lt;/p></description></item><item><title>Note on Extremal Perturbation ICCV 2019</title><link>https://lif323.github.io/posts/post-10/note-on-extremal-perturbation-iccv-2019/</link><pubDate>Fri, 21 Jun 2024 16:16:47 +0800</pubDate><guid>https://lif323.github.io/posts/post-10/note-on-extremal-perturbation-iccv-2019/</guid><description>$$
m_{\lambda, \beta} = \argmax_{m} \Phi(m \otimes x) - \lambda \|m\|_{1} - \beta \mathcal{S}(m).
$$&lt;p>They believe that the meaning of the trade-off of this formulation is unclear.
In particular, choosing different $\lambda$ and $\beta$ will result in different masks without a clear way of comparing them.&lt;/p>
$$
m_{a} = \argmax_{m: \|m\|_{1} = \alpha |\Omega|, m \in \mathcal{M}} \Phi(m \otimes x)
$$&lt;p>
They think that the resulting mask is a function of the chosen area $a$ only.&lt;/p></description></item><item><title>Cross Entropy</title><link>https://lif323.github.io/posts/post-8/cross_entropy/</link><pubDate>Thu, 20 Jun 2024 15:57:12 +0800</pubDate><guid>https://lif323.github.io/posts/post-8/cross_entropy/</guid><description>$$
H(p, q) = - \sum_{c \in C} p(c)\log q(c)
$$&lt;p>The &amp;rsquo;true&amp;rsquo; distribution is usually expressed in terms of a one-hot distribution.&lt;/p>
&lt;p>Suppose that the true label of an instance is B. The one-hot distribution for this instance is:&lt;/p>
&lt;pre tabindex="0">&lt;code>Pr(Class A) Pr(Class B) Pr(Class C)
0.0 1.0 0.0
&lt;/code>&lt;/pre>&lt;p>Suppose a machine learning algorithm predicts the following probability distribution:&lt;/p>
&lt;pre tabindex="0">&lt;code>Pr(Class A) Pr(Class B) Pr(Class C)
0.228 0.619 0.153
&lt;/code>&lt;/pre>&lt;p>The cross-entropy loss of this case is 0.479:&lt;/p></description></item><item><title>Note on MeaningfulPerturbation ICCV 2017</title><link>https://lif323.github.io/posts/post-7/note-on-meaningfulperturbation-iccv-2017/</link><pubDate>Tue, 18 Jun 2024 22:07:07 +0800</pubDate><guid>https://lif323.github.io/posts/post-7/note-on-meaningfulperturbation-iccv-2017/</guid><description>&lt;p>As far as I know, this paper is the first to propose providing explanations for black-box models by learning a mask. Additionally, this paper presents an interesting perspective, which views explaining black-box models as a form of meta-learning. Specifically, an explanation is a rule that predicts the block-box model&amp;rsquo;s output for a given input.&lt;/p>
&lt;p>A significant advantage of formulating explanations as meta learning is that the fidelity of the explanations can be measured as prediction accuracy.&lt;/p></description></item><item><title>Note on ConceptBottleneckModels ICML 2020</title><link>https://lif323.github.io/posts/post-6/note-on-conceptbottleneckmodels-icml-2020/</link><pubDate>Fri, 14 Jun 2024 21:13:21 +0800</pubDate><guid>https://lif323.github.io/posts/post-6/note-on-conceptbottleneckmodels-icml-2020/</guid><description>&lt;p>In this paper, they systematically study different ways of learning concept bottleneck models. Let $L_{c_j}$ be a loss function that measures the discrepancy between the predicted and true j-th concept, and let $L_Y$ measure the discrepancy between predicted and true targets. They consider the following ways to learn a concept bottleneck model $(\hat{f}, \hat{g})$:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>The &lt;em>independent bottleneck&lt;/em> learns $\hat{f}$ and $\hat{g}$ independently: $\hat{f} = \argmin_{f}\sum_{i}L_{Y}(f(c^{i}); y^i)$, and $\hat{g} = \argmin_{g}\sum_{i,j}L_{C_j}(g_j(x^i);c_j^i)$. While $\hat{f}$ is &lt;strong>trained using the true $c$, at test time it still takes $\hat{g}(x)$ as input&lt;/strong>.&lt;/p></description></item><item><title>Note on SaliencyGuidedTraining NeurIPS 2021</title><link>https://lif323.github.io/posts/post-5/note-on-saliencyguidedtraining-neurips-2021/</link><pubDate>Tue, 11 Jun 2024 16:08:08 +0800</pubDate><guid>https://lif323.github.io/posts/post-5/note-on-saliencyguidedtraining-neurips-2021/</guid><description>&lt;p>This paper proposes a saliency-guided training procedure aimed at reducing &lt;strong>noisy gradients&lt;/strong>, which can lead to unfaithful feature attributions, while maintaining the predictive performance of the model.&lt;/p>
$$
\tilde{X} = M_{k}(S(\nabla_{X}f_{\theta}(X), X),
$$&lt;p>
where $S(\cdot)$ is a sorting function and $M(\cdot)$ replaces the bottom $k$ elements with a mask distribution based on the order provided by $S(\cdot)$.&lt;/p>
$$
\text{minimize}_{\theta} \frac{1}{n} \sum_{i=1}^{n} \left[\mathcal{L}(f_{\theta}(X_i), y_i) + \lambda D_{KL}(f_{\theta}(X_i) || f_{\theta}(\tilde{X_i}) ) \right]
$$&lt;p>I believe the superior performance of SGT comes from its ability to reduce the impact of unimportant features during the training phase.&lt;/p></description></item><item><title>Note on DynaMask ICML 2021</title><link>https://lif323.github.io/posts/post-4/</link><pubDate>Fri, 07 Jun 2024 22:36:34 +0800</pubDate><guid>https://lif323.github.io/posts/post-4/</guid><description>$$
x'_{t,i} = x_{t,i} * M_{t, i} + b_{t,i} * (1 - M_{t, i})
$$&lt;p>
where $b_{t,i}$ is usually generated based on the data distribution.&lt;/p>
$$
L(M) = \sum_{c=1}^{C} f(x) \log f(x')
$$&lt;h2 id="references">References&lt;/h2>
&lt;p>CrabbÃ©, J., &amp;amp; Van Der Schaar, M. (2021, July). Explaining time series predictions with dynamic masks. In International Conference on Machine Learning (pp. 2166-2177). PMLR.&lt;/p></description></item><item><title>Note on GSAT ICML 2022</title><link>https://lif323.github.io/posts/post-3/</link><pubDate>Thu, 30 May 2024 17:40:58 +0800</pubDate><guid>https://lif323.github.io/posts/post-3/</guid><description>&lt;p>&lt;img loading="lazy" src="./icml-2022-gsat.png" alt="alt text" />
&lt;/p>
&lt;p>This paper introduces GSAT methods for generating explanations in graph learning. GSAT is composed of the following parts:&lt;/p>
&lt;p>$G = (A, X)$ $A$ is the adjacency matrix and $X$ is the node attributes. Let $V$ and $E$ denote the node set and the edge set.&lt;/p>
&lt;p>Firstly, the extractor $g_{\phi}$ encodes the input graph $G$ via a GNN into a set of node representations $(h_u | u \in X)$. For any two nodes $v$ and $u$, an MLP layer maps the concatenation of their node representations to the probability $p_{u, v} \in [0, 1]$ of an edge existing between that corresponding pair of nodes.&lt;/p></description></item><item><title>Gumbel Softmax</title><link>https://lif323.github.io/posts/post-2/</link><pubDate>Thu, 18 Apr 2024 11:24:57 +0800</pubDate><guid>https://lif323.github.io/posts/post-2/</guid><description>&lt;h1 id="what-is-gumbel-softmax">What is Gumbel-softmax?&lt;/h1>
&lt;p>Gumbel-softmax is an efficient tool to sample from a categorical distribution.&lt;/p>
&lt;p>Let $z$ be a categorical variable with class probabilities $\pi_1$, $\pi_2$, $\cdots$, $\pi_k$.&lt;/p>
$$
z = \text{one\_hot}(\argmax_{i}\left[g_i + log \pi_i\right])
$$&lt;p>
where $g_1$, $\cdots$, $g_k$ are i.i.d samples drawn from Gumbel(0, 1).
The Gumbel(0, 1) distribution can be sampled by drawing $u \sim \text{Uniform}(0, 1)$ and computing $g = - \log (- \log (u))$.&lt;/p>
&lt;p>However, due to the use of argmax, the Gumbel-Max trick is not-differentiable operation.&lt;/p></description></item><item><title>Notes on the Paper NeurIPS 2023 Evaluating Post Hoc Explanations for Graph Neural Networks via Robustness Analysis</title><link>https://lif323.github.io/posts/post-1/notes-on-the-paper-neurips-2023-evaluating-post-hoc-explanations-for-graph-neural-networks-via-robustness-analysis/</link><pubDate>Sat, 13 Apr 2024 12:45:24 +0800</pubDate><guid>https://lif323.github.io/posts/post-1/notes-on-the-paper-neurips-2023-evaluating-post-hoc-explanations-for-graph-neural-networks-via-robustness-analysis/</guid><description>&lt;p>This paper focuses on the performance of evaluation methods of Post-hoc Explanations. Current prevailing evalution methods mainly inlcudes two types: Feature Removal methods and Generation-based methods.&lt;/p>
&lt;p>This paper hones in on the evaluation of the effectiveness of Post-hoc Explanation evaluation methods. Predominantly, current methodologies fall into two primary classifications: Feature Removal and Generation-based approaches.&lt;/p>
&lt;p>Feature Removal operates on the principle of excising salient features identified through explanation mechanisms. The merit of such explanations is quantified by the variance in the model&amp;rsquo;s output pre and post-feature extrication. The larger the resultant decrement in model performance, which intimates the importance of the deleted features to the model&amp;rsquo;s functionality, the more adept the explanation method is deemed.&lt;/p></description></item></channel></rss>