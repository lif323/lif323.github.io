<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Posts on lif323</title><link>https://lif323.github.io/posts/</link><description>Recent content in Posts on lif323</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Thu, 20 Jun 2024 15:57:12 +0800</lastBuildDate><atom:link href="https://lif323.github.io/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Cross Entropy</title><link>https://lif323.github.io/posts/post-8/cross_entropy/</link><pubDate>Thu, 20 Jun 2024 15:57:12 +0800</pubDate><guid>https://lif323.github.io/posts/post-8/cross_entropy/</guid><description>Cross-entropy is commonly used to quantify the difference between two probability distributions. In machine learning, its definition for one instance is as follows: $$ H(p, q) = - \sum_{c \in C} p(c)\log q(c) $$ The &amp;rsquo;true&amp;rsquo; distribution is usually expressed in terms of a one-hot distribution.
Suppose that the true label of an instance is B. The one-hot distribution for this instance is:
Pr(Class A) Pr(Class B) Pr(Class C) 0.0 1.</description></item><item><title>Note on MeaningfulPerturbation ICCV 2017</title><link>https://lif323.github.io/posts/post-7/note-on-meaningfulperturbation-iccv-2017/</link><pubDate>Tue, 18 Jun 2024 22:07:07 +0800</pubDate><guid>https://lif323.github.io/posts/post-7/note-on-meaningfulperturbation-iccv-2017/</guid><description>As far as I know, this paper is the first to propose providing explanations for black-box models by learning a mask. Additionally, this paper presents an interesting perspective, which views explaining black-box models as a form of meta-learning. Specifically, an explanation is a rule that predicts the block-box model&amp;rsquo;s output for a given input.
A significant advantage of formulating explanations as meta learning is that the fidelity of the explanations can be measured as prediction accuracy.</description></item><item><title>Note on ConceptBottleneckModels ICML 2020</title><link>https://lif323.github.io/posts/post-6/note-on-conceptbottleneckmodels-icml-2020/</link><pubDate>Fri, 14 Jun 2024 21:13:21 +0800</pubDate><guid>https://lif323.github.io/posts/post-6/note-on-conceptbottleneckmodels-icml-2020/</guid><description>In this paper, they systematically study different ways of learning concept bottleneck models. Let $L_{c_j}$ be a loss function that measures the discrepancy between the predicted and true j-th concept, and let $L_Y$ measure the discrepancy between predicted and true targets. They consider the following ways to learn a concept bottleneck model $(\hat{f}, \hat{g})$:
The independent bottleneck learns $\hat{f}$ and $\hat{g}$ independently: $\hat{f} = \argmin_{f}\sum_{i}L_{Y}(f(c^{i}); y^i)$, and $\hat{g} = \argmin_{g}\sum_{i,j}L_{C_j}(g_j(x^i);c_j^i)$. While $\hat{f}$ is trained using the true $c$, at test time it still takes $\hat{g}(x)$ as input.</description></item><item><title>Note on SaliencyGuidedTraining NeurIPS 2021</title><link>https://lif323.github.io/posts/post-5/note-on-saliencyguidedtraining-neurips-2021/</link><pubDate>Tue, 11 Jun 2024 16:08:08 +0800</pubDate><guid>https://lif323.github.io/posts/post-5/note-on-saliencyguidedtraining-neurips-2021/</guid><description>This paper proposes a saliency-guided training procedure aimed at reducing noisy gradients, which can lead to unfaithful feature attributions, while maintaining the predictive performance of the model.
During saliency-guided training, for every input $X$, they create a new input $\tilde{X}$ by masking the features with low gradient values as follows: $$ \tilde{X} = M_{k}(S(\nabla_{X}f_{\theta}(X), X), $$ where $S(\cdot)$ is a sorting function and $M(\cdot)$ replaces the bottom $k$ elements with a mask distribution based on the order provided by $S(\cdot)$.</description></item><item><title>Note on DynaMask ICML 2021</title><link>https://lif323.github.io/posts/post-4/</link><pubDate>Fri, 07 Jun 2024 22:36:34 +0800</pubDate><guid>https://lif323.github.io/posts/post-4/</guid><description>This paper is among the earliest works to learn a mask to measure the importance of each feature. Assuming a mask $M$, where $M_{t,i} \in [0, 1] $. When $M_{t, i} = 0$, this feature is irrelevant for black-box prediction. Conversely, when $M_{t, i}=1$, this feature is important for black-box prediction. The input $x$ is perturbed based on $M_{t, i}$. A simple perturbation method is as follows: $$ x'_{t,i} = x_{t,i} * M_{t, i} + b_{t,i} * (1 - M_{t, i}) $$ where $b_{t,i}$ is usually generated based on the data distribution.</description></item><item><title>Note on GSAT ICML 2022</title><link>https://lif323.github.io/posts/post-3/</link><pubDate>Thu, 30 May 2024 17:40:58 +0800</pubDate><guid>https://lif323.github.io/posts/post-3/</guid><description>This paper introduces GSAT methods for generating explanations in graph learning. GSAT is composed of the following parts:
$G = (A, X)$ $A$ is the adjacency matrix and $X$ is the node attributes. Let $V$ and $E$ denote the node set and the edge set.
Firstly, the extractor $g_{\phi}$ encodes the input graph $G$ via a GNN into a set of node representations $(h_u | u \in X)$. For any two nodes $v$ and $u$, an MLP layer maps the concatenation of their node representations to the probability $p_{u, v} \in [0, 1]$ of an edge existing between that corresponding pair of nodes.</description></item><item><title>Gumbel Softmax</title><link>https://lif323.github.io/posts/post-2/gumbel-softmax/</link><pubDate>Thu, 18 Apr 2024 11:24:57 +0800</pubDate><guid>https://lif323.github.io/posts/post-2/gumbel-softmax/</guid><description>What is Gubmel-softmax? Gumbel-softmax is an efficient gradient estimator for the non-differentiable sample from a categorical distribution.
Let $z$ be a categorical variable with class probabilities $\pi_1$, $\pi_2$, $\cdots$, $\pi_k$.
The Gumbel-Max trick is an efficient way to draw samples $z$ from a categorical distribution with class probabilities $\pi$: $$ z = \text{one\_hot}(\argmax_{i}\left[g_i + log \pi_i\right]) $$ where $g_1$, $\cdots$, $g_k$ are i.i.d samples drawn from Gumbel(0, 1). The Gubel(0, 1) distribution can be sampled by drawing $u \sim \text{Uniform}(0, 1)$ and computing $g = - \log (- \log (u))$.</description></item><item><title>Notes on the Paper NeurIPS 2023 Evaluating Post Hoc Explanations for Graph Neural Networks via Robustness Analysis</title><link>https://lif323.github.io/posts/post-1/notes-on-the-paper-neurips-2023-evaluating-post-hoc-explanations-for-graph-neural-networks-via-robustness-analysis/</link><pubDate>Sat, 13 Apr 2024 12:45:24 +0800</pubDate><guid>https://lif323.github.io/posts/post-1/notes-on-the-paper-neurips-2023-evaluating-post-hoc-explanations-for-graph-neural-networks-via-robustness-analysis/</guid><description>This paper focuses on the performance of evaluation methods of Post-hoc Explanations. Current prevailing evalution methods mainly inlcudes two types: Feature Removal methods and Generation-based methods.
This paper hones in on the evaluation of the effectiveness of Post-hoc Explanation evaluation methods. Predominantly, current methodologies fall into two primary classifications: Feature Removal and Generation-based approaches.
Feature Removal operates on the principle of excising salient features identified through explanation mechanisms. The merit of such explanations is quantified by the variance in the model&amp;rsquo;s output pre and post-feature extrication.</description></item></channel></rss>