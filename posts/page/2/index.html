<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Posts | lif323</title>
<meta name=keywords content><meta name=description content="Posts - lif323"><meta name=author content><link rel=canonical href=https://lif323.github.io/posts/><link crossorigin=anonymous href=/assets/css/stylesheet.1d23bc8761bb8a516d01801663a24eeedbf97720019a7ddc0fc9ff29a2a8730d.css integrity="sha256-HSO8h2G7ilFtAYAWY6JO7tv5dyABmn3cD8n/KaKocw0=" rel="preload stylesheet" as=style><link rel=icon href=https://lif323.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://lif323.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://lif323.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://lif323.github.io/apple-touch-icon.png><link rel=mask-icon href=https://lif323.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://lif323.github.io/posts/index.xml><link rel=alternate hreflang=en href=https://lif323.github.io/posts/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"\\[",right:"\\]",display:!0},{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})})</script><meta property="og:title" content="Posts"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="https://lif323.github.io/posts/"><meta name=twitter:card content="summary"><meta name=twitter:title content="Posts"><meta name=twitter:description content><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://lif323.github.io/posts/"}]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://lif323.github.io/ accesskey=h title="lif323 (Alt + H)">lif323</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><header class=page-header><h1>Posts</h1></header><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Maximum Likelihood Estimation and KL Divergence</h2></header><div class=entry-content><p>The core idea of maximum likelihood estimation is to assume that we draw samples $\{x^1, x^2, \cdots, x^m\}$ from the data distribution $P_{\text{data}}(x)$ and calculate the probability $P_{\theta}(x^i)$ of observing each sample $x^i$. The objective is to find the parameters $\theta$ that maximize the likelihood of observing all the samples. The optimization objective is formulated as follows:
$$ \theta^{*} = \argmax_{\theta} \prod_{i=1}^{m} P_{\theta}(x^i) $$$$ \begin{aligned} \theta^{*} &= \argmax_{\theta} \prod_{i=1}^{m} P_{\theta}(x^i)\\ & = \argmax_{\theta} \log \prod_{i=1}^{m} P_{\theta}(x^i)\\ & = \argmax_{\theta} \sum_{i=1}^{m} \log P_{\theta}(x^i)\\ & \approx \argmax_{\theta} E_{x \sim P_{\text{data}}} \left[ \log P_{\theta}(x^i) \right]\\ & = \argmax_{\theta} \int_{x} P_{\text{data}}(x) \log P_{\theta}(x) dx\\ & = \argmax_{\theta} \int_{x} P_{\text{data}}(x) \log P_{\theta}(x) dx - \underbrace{\int_{x} P_{\text{data}}(x) \log P_{\text{data}}(x) dx}_{\text{not related to $\theta$}}\\ & = \argmax_{\theta} \int_{x} P_{\text{data}}(x) \log \frac{P_{\theta}(x)} {P_{\text{data}}(x)} dx\\ & = \argmax_{\theta} -1 \cdot \int_{x} P_{\text{data}}(x) \log \frac{P_{\text{data}}(x)} {P_{\theta}(x)} dx\\ & = \argmin_{\theta} KL(P_{\text{data}}|| P_{\theta}) \end{aligned} $$References https://www.youtube.com/watch?v=67_M2qP5ssY&amp;list=PLJV_el3uVTsNi7PgekEUFsyVllAJXRsP-
...</p></div><footer class=entry-footer><span title='2024-09-04 10:43:33 +0800 +0800'>September 4, 2024</span></footer><a class=entry-link aria-label="post link to Maximum Likelihood Estimation and KL Divergence" href=https://lif323.github.io/posts/post-24/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>CEM NeurIPS 2018</h2></header><div class=entry-content><p>This paper proposes a Contrastive Explanations method(CEM). They define a type of explanation where, given an input, the goal is to identify which features are both minimal and sufficient to justify its classification as well as which features are minimal and necessarily absent.
Pertinent Negatives (PN) refers to the importance of missing features in model predictions. It essentially represents a counterfactual explanation. $\mathbf{x}_0 + \boldsymbol{\delta}$ denotes the counterfactual instance. A pertinent negative can be identified by solving the following optimization problem:
...</p></div><footer class=entry-footer><span title='2024-08-26 10:44:24 +0800 +0800'>August 26, 2024</span></footer><a class=entry-link aria-label="post link to CEM NeurIPS 2018" href=https://lif323.github.io/posts/post-20/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Note Integrated Directional Gradients</h2></header><div class=entry-content><p>This paper proposes the Integrated Directional Gradients (IDG) to computing feature group attribution. One important thing is to find the family of meaningful feature subsets which is defined by the domain related methods, such as the constituency parse tree for NLP.
$$ z_i^S = \begin{cases} x_i - b_i & \text {if} a_i \in S\\ 0 & \text{ otherwise} \end{cases} $$$$ \nabla_{S}f(x) = \nabla f(x) \cdot \hat{z}^{S} \quad \hat{z}^{S} = \frac{z^{S}}{\Vert z^{S} \Vert} $$$$ \text{IDG}(S) = \int_{\alpha = 0}^1 \nabla_{S}f(b + \alpha (x - b)) d \alpha $$ The dividend $d(S)$ of the feature subset $S$ is computed by normalizing the absolute value of $IDG(S)$ over all meaningful subsets.
...</p></div><footer class=entry-footer><span title='2024-08-05 16:07:54 +0800 +0800'>August 5, 2024</span></footer><a class=entry-link aria-label="post link to Note Integrated Directional Gradients" href=https://lif323.github.io/posts/post-17/note-integrated-directional-gradients/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Note Ranking Problems</h2></header><div class=entry-content><p>In machine learning, most problems are classified as Classification or Regression. However, this categorization is not always suitable for real-world problems. In some tasks, the classes are imbalanced but exhibit some weak order. In these cases, the loss functions for Classification and Regression do not perform well. This blog will apply the less commonly used margin ranking loss to address this issue.
Here is an example from the UCI Wine Quality Dataset from Cortez et al. Suppose we are a wine supplier and we find that a high review in some wine magazines increases the demand for a particular wine. Therefore, we want to predict the review score based on some measurable features of the wine. This way, we can increase the stock of the corresponding wine in advance, before the demand rises.
...</p></div><footer class=entry-footer><span title='2024-07-14 09:09:56 +0800 +0800'>July 14, 2024</span></footer><a class=entry-link aria-label="post link to Note Ranking Problems" href=https://lif323.github.io/posts/post-16/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Note Discretized Integrated Gradients EMNLP 2021</h2></header><div class=entry-content><p>This paper proposes the Discretized Integrated Gradients (DIG) method, which effectively applies the Integrated Gradients(IG) method to the word embedding space.
Integrated Gradient (IG) measures the importance of features by using the average of model gradients at interpolation points along a straight path in the input space.
However, in the word embedding space, due to its discrete nature, interpolation points may not accurately represent textual data. In particular cases, interpolation points could be outliers.
...</p></div><footer class=entry-footer><span title='2024-07-08 15:58:29 +0800 +0800'>July 8, 2024</span></footer><a class=entry-link aria-label="post link to Note Discretized Integrated Gradients EMNLP 2021" href=https://lif323.github.io/posts/post-15/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Note Learning Perturbations ICML 2023</h2></header><div class=entry-content><p>This paper improves existing methods that provide explanations using trainable masks. Inspired by methods for static data, the current approach uses fixed perturbations. However, this paper argues that this approach may not be suitable for time series data. In this paper, their method not only has a trainable mask but also trainable perturbations.
$$ \Phi(\mathbf{x}, \mathbf{m}) = \mathbf{m} \times \mathbf{x} + (1 - \mathbf{m}) \times g(\mathbf{x}), $$ where $g(\mathbf{x})$ is a function of the input. A possible definition is $g(\mathbf{x}) = \frac{1}{W}\sum_{t'=t-W}^{t}x_{t'}$.
...</p></div><footer class=entry-footer><span title='2024-07-07 15:00:05 +0800 +0800'>July 7, 2024</span></footer><a class=entry-link aria-label="post link to Note Learning Perturbations ICML 2023" href=https://lif323.github.io/posts/post-14/note-learningperturbations-2023/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>KL Divergence and Cross Entropy</h2></header><div class=entry-content><p>$$ H(P, Q) = - \sum_{x}p(x)\log Q(x) $$$$ KL(P|Q) = \sum_{x}P(x)\log\frac{P(x)}{Q(x)} $$$$ S(v) = - \sum_{i}p(v_i)\log p(v_i), $$ where $p(v_i)$ represents the probability of state $v_i$. From the perspective of information theory, $S(v)$ is the information required to remove system uncertainty.
$$ KL(A|B) = \sum_{i}p_A(v_i)\log p_A(v_i) - p_A(v_i)\log p_B(v_i), $$ where the first term of the right hand side is the entropy of distribution $A$, the second term can be interpreted as the expectation of distribution $B$ in terms of $A$. The $KL(A|B)$ describes how different $B$ is from $A$ from the perspective of $A$.
...</p></div><footer class=entry-footer><span title='2024-06-24 21:37:58 +0800 +0800'>June 24, 2024</span></footer><a class=entry-link aria-label="post link to KL Divergence and Cross Entropy" href=https://lif323.github.io/posts/post-12/kl-divergence-and-cross-entropy/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Note on RealTimeImageSaliency NeurIPS 2017</h2></header><div class=entry-content><p>$$ \Phi(X, M) = X \odot M + A \odot (1 - M), $$ where $X$ is the original input, and $A$ is a reference input, which is usually a highly blurred version of $X$.
$$ TV(M) = \sum_{i,j}(M_{ij} - M_{ij+1})^2 + \sum_{i,j}(M_{ij} - M_{i+1j})^2 $$$$ L(M) = \lambda_1 TV(M) + \lambda_2 AV(M) - \log (f_{c}(\Phi(X, M))) + \lambda_3 f_c(\Phi(X, 1- M))^{\lambda_4} $$ The third term makes sure that the classifier is able to recognize the selected class from the preserved region. It is worth noting that the last term ensures that the probability of the selected class after the salient region is removed, is low. Setting $\lambda_4$ to a value smaller than 1 helps reduce this probability to very small values.
...</p></div><footer class=entry-footer><span title='2024-06-24 15:59:28 +0800 +0800'>June 24, 2024</span></footer><a class=entry-link aria-label="post link to Note on RealTimeImageSaliency NeurIPS 2017" href=https://lif323.github.io/posts/post-11/note-on-realtimeimagesaliency-neurips-2017/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Note on Extremal Perturbation ICCV 2019</h2></header><div class=entry-content><p>$$ m_{\lambda, \beta} = \argmax_{m} \Phi(m \otimes x) - \lambda \|m\|_{1} - \beta \mathcal{S}(m). $$They believe that the meaning of the trade-off of this formulation is unclear. In particular, choosing different $\lambda$ and $\beta$ will result in different masks without a clear way of comparing them.
$$ m_{a} = \argmax_{m: \|m\|_{1} = \alpha |\Omega|, m \in \mathcal{M}} \Phi(m \otimes x) $$ They think that the resulting mask is a function of the chosen area $a$ only.
...</p></div><footer class=entry-footer><span title='2024-06-21 16:16:47 +0800 +0800'>June 21, 2024</span></footer><a class=entry-link aria-label="post link to Note on Extremal Perturbation ICCV 2019" href=https://lif323.github.io/posts/post-10/note-on-extremal-perturbation-iccv-2019/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Cross Entropy</h2></header><div class=entry-content><p>$$ H(p, q) = - \sum_{c \in C} p(c)\log q(c) $$The ’true’ distribution is usually expressed in terms of a one-hot distribution.
Suppose that the true label of an instance is B. The one-hot distribution for this instance is:
Pr(Class A) Pr(Class B) Pr(Class C) 0.0 1.0 0.0 Suppose a machine learning algorithm predicts the following probability distribution:
Pr(Class A) Pr(Class B) Pr(Class C) 0.228 0.619 0.153 The cross-entropy loss of this case is 0.479:
...</p></div><footer class=entry-footer><span title='2024-06-20 15:57:12 +0800 +0800'>June 20, 2024</span></footer><a class=entry-link aria-label="post link to Cross Entropy" href=https://lif323.github.io/posts/post-8/cross_entropy/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://lif323.github.io/posts/>«&nbsp;Prev&nbsp;
</a><a class=next href=https://lif323.github.io/posts/page/3/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2024 <a href=https://lif323.github.io/>lif323</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>