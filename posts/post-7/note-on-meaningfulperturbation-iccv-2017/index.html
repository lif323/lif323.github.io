<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Note on MeaningfulPerturbation ICCV 2017 | lif323</title>
<meta name=keywords content><meta name=description content="As far as I know, this paper is the first to propose providing explanations for black-box models by learning a mask. Additionally, this paper presents an interesting perspective, which views explaining black-box models as a form of meta-learning. Specifically, an explanation is a rule that predicts the block-box model&rsquo;s output for a given input.
A significant advantage of formulating explanations as meta learning is that the fidelity of the explanations can be measured as prediction accuracy."><meta name=author content><link rel=canonical href=https://lif323.github.io/posts/post-7/note-on-meaningfulperturbation-iccv-2017/><link crossorigin=anonymous href=/assets/css/stylesheet.1d23bc8761bb8a516d01801663a24eeedbf97720019a7ddc0fc9ff29a2a8730d.css integrity="sha256-HSO8h2G7ilFtAYAWY6JO7tv5dyABmn3cD8n/KaKocw0=" rel="preload stylesheet" as=style><link rel=icon href=https://lif323.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://lif323.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://lif323.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://lif323.github.io/apple-touch-icon.png><link rel=mask-icon href=https://lif323.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://lif323.github.io/posts/post-7/note-on-meaningfulperturbation-iccv-2017/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"\\[",right:"\\]",display:!0},{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})})</script><meta property="og:title" content="Note on MeaningfulPerturbation ICCV 2017"><meta property="og:description" content="As far as I know, this paper is the first to propose providing explanations for black-box models by learning a mask. Additionally, this paper presents an interesting perspective, which views explaining black-box models as a form of meta-learning. Specifically, an explanation is a rule that predicts the block-box model&rsquo;s output for a given input.
A significant advantage of formulating explanations as meta learning is that the fidelity of the explanations can be measured as prediction accuracy."><meta property="og:type" content="article"><meta property="og:url" content="https://lif323.github.io/posts/post-7/note-on-meaningfulperturbation-iccv-2017/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-06-18T22:07:07+08:00"><meta property="article:modified_time" content="2024-06-18T22:07:07+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Note on MeaningfulPerturbation ICCV 2017"><meta name=twitter:description content="As far as I know, this paper is the first to propose providing explanations for black-box models by learning a mask. Additionally, this paper presents an interesting perspective, which views explaining black-box models as a form of meta-learning. Specifically, an explanation is a rule that predicts the block-box model&rsquo;s output for a given input.
A significant advantage of formulating explanations as meta learning is that the fidelity of the explanations can be measured as prediction accuracy."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://lif323.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Note on MeaningfulPerturbation ICCV 2017","item":"https://lif323.github.io/posts/post-7/note-on-meaningfulperturbation-iccv-2017/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Note on MeaningfulPerturbation ICCV 2017","name":"Note on MeaningfulPerturbation ICCV 2017","description":"As far as I know, this paper is the first to propose providing explanations for black-box models by learning a mask. Additionally, this paper presents an interesting perspective, which views explaining black-box models as a form of meta-learning. Specifically, an explanation is a rule that predicts the block-box model\u0026rsquo;s output for a given input.\nA significant advantage of formulating explanations as meta learning is that the fidelity of the explanations can be measured as prediction accuracy.\n","keywords":[],"articleBody":"As far as I know, this paper is the first to propose providing explanations for black-box models by learning a mask. Additionally, this paper presents an interesting perspective, which views explaining black-box models as a form of meta-learning. Specifically, an explanation is a rule that predicts the block-box model’s output for a given input.\nA significant advantage of formulating explanations as meta learning is that the fidelity of the explanations can be measured as prediction accuracy.\nThey consider two approaches to summarize compactly the effect of deleting image regions in order to explain the behavior of the black box.\nOne method is the ‘deletion game’, which aims to find the smallest deletion mask $m$ that causes the scores $f_{c}(\\Phi(x_0, m))\\ll f_{c}(x_0)$ to drop significantly, where c is the target class.\n$$ m^*=\\underset{m \\in[0,1]^{\\Lambda}}{\\operatorname{argmin}} \\lambda\\|\\mathbf{1}-m\\|_1+f_c\\left(\\Phi\\left(x_0 ; m\\right)\\right) $$The other method is the ‘preservation game’, where the goal is to find the smallest preservation mask $m$ that must be retain to preserve the score $f_{c}(\\Phi(x_0;m))\\ge f_{c}(x_0)$:\n$$m^{*} = \\argmin_{m}\\lambda\\|m\\|_1 - f_{c}(\\Phi(x_0;m))$$$$ \\mathbb{E}_\\tau\\left[f_c(\\Phi(x_0, m)) + \\tau\\right] $$$$ TV(m) = \\sum_{i=1}^{N} \\sum_{j=1}^{M}\\left(|m_{i, j} - m_{i, j+1}| + |m_{i, j} - m_{i+1, j}|\\right) $$References Fong, R. C., \u0026 Vedaldi, A. (2017). Interpretable explanations of black boxes by meaningful perturbation. In Proceedings of the IEEE international conference on computer vision (pp. 3429-3437).\nDabkowski, P., \u0026 Gal, Y. (2017). Real time image saliency for black box classifiers. Advances in neural information processing systems, 30.\n","wordCount":"237","inLanguage":"en","datePublished":"2024-06-18T22:07:07+08:00","dateModified":"2024-06-18T22:07:07+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://lif323.github.io/posts/post-7/note-on-meaningfulperturbation-iccv-2017/"},"publisher":{"@type":"Organization","name":"lif323","logo":{"@type":"ImageObject","url":"https://lif323.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://lif323.github.io/ accesskey=h title="lif323 (Alt + H)">lif323</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Note on MeaningfulPerturbation ICCV 2017</h1><div class=post-meta><span title='2024-06-18 22:07:07 +0800 +0800'>June 18, 2024</span></div></header><div class=post-content><p>As far as I know, this paper is the first to propose providing explanations for black-box models by learning a mask. Additionally, this paper presents an interesting perspective, which views explaining black-box models as a form of meta-learning. Specifically, an explanation is a rule that predicts the block-box model&rsquo;s output for a given input.</p><p>A significant advantage of formulating explanations as meta learning is that the fidelity of the explanations can be measured as prediction accuracy.</p><p>They consider two approaches to summarize compactly the effect of deleting image regions in order to explain the behavior of the black box.</p><p>One method is the &lsquo;deletion game&rsquo;, which aims to find the smallest deletion mask $m$ that causes the scores $f_{c}(\Phi(x_0, m))\ll f_{c}(x_0)$ to drop significantly, where c is the target class.</p>$$
m^*=\underset{m \in[0,1]^{\Lambda}}{\operatorname{argmin}} \lambda\|\mathbf{1}-m\|_1+f_c\left(\Phi\left(x_0 ; m\right)\right)
$$<p>The other method is the &lsquo;preservation game&rsquo;, where the goal is to find the smallest preservation mask $m$ that must be retain to preserve the score $f_{c}(\Phi(x_0;m))\ge f_{c}(x_0)$:</p>$$m^{*} = \argmin_{m}\lambda\|m\|_1 - f_{c}(\Phi(x_0;m))$$$$
\mathbb{E}_\tau\left[f_c(\Phi(x_0, m)) + \tau\right]
$$$$
TV(m) = \sum_{i=1}^{N} \sum_{j=1}^{M}\left(|m_{i, j} - m_{i, j+1}| + |m_{i, j} - m_{i+1, j}|\right)
$$<h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>Fong, R. C., & Vedaldi, A. (2017). Interpretable explanations of black boxes by meaningful perturbation. In Proceedings of the IEEE international conference on computer vision (pp. 3429-3437).</p><p>Dabkowski, P., & Gal, Y. (2017). Real time image saliency for black box classifiers. Advances in neural information processing systems, 30.</p></div><footer class=post-footer><ul class=post-tags></ul></footer><script src=https://giscus.app/client.js data-repo=lif323/lif323.github.io data-repo-id=R_kgDOLtjKFA data-category=Announcements data-category-id=DIC_kwDOLtjKFM4Ce08J data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=light data-lang=en crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2025 <a href=https://lif323.github.io/>lif323</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>