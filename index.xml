<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>lif323</title><link>https://lif323.github.io/</link><description>Recent content on lif323</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Mon, 08 Jul 2024 15:58:29 +0800</lastBuildDate><atom:link href="https://lif323.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Note Discretized Integrated Gradients EMNLP 2021</title><link>https://lif323.github.io/posts/post-15/note-discretized-integrated-gradients-emnlp-2021/</link><pubDate>Mon, 08 Jul 2024 15:58:29 +0800</pubDate><guid>https://lif323.github.io/posts/post-15/note-discretized-integrated-gradients-emnlp-2021/</guid><description>This paper proposes the Discretized Integrated Gradients (DIG) method, which effectively applies the Integrated Gradients(IG) method to the word embedding space.
Integrated Gradient (IG) measures the importance of features by using the average of model gradients at interpolation points along a straight path in the input space.
However, in the word embedding space, due to its discrete nature, interpolation points may not accurately represent textual data. In particular cases, interpolation points could be outliers.</description></item><item><title>KL Divergence and Cross Entropy</title><link>https://lif323.github.io/posts/post-12/kl-divergence-and-cross-entropy/</link><pubDate>Mon, 24 Jun 2024 21:37:58 +0800</pubDate><guid>https://lif323.github.io/posts/post-12/kl-divergence-and-cross-entropy/</guid><description>The defination of Cross-entropy is as follows: $$ H(P, Q) = - \sum_{x}p(x)\log Q(x) $$ The KL divergence is defined as follows: $$ KL(P|Q) = \sum_{x}P(x)\log\frac{P(x)}{Q(x)} $$ We first introduce the definition of information entropy: $$ S(v) = - \sum_{i}p(v_i)\log p(v_i), $$ where $p(v_i)$ represents the probability of state $v_i$. From the perspective of information theory, $S(v)$ is the information required to remove system uncertainty.
The formula for KL divergence can be further transformed into the following form: $$ KL(A|B) = \sum_{i}p_A(v_i)\log p_A(v_i) - p_A(v_i)\log p_B(v_i), $$ where the first term of the right hand side is the entropy of distribution $A$, the second term can be interpreted as the expectation of distribution $B$ in terms of $A$.</description></item><item><title>Note on RealTimeImageSaliency NeurIPS 2017</title><link>https://lif323.github.io/posts/post-11/note-on-realtimeimagesaliency-neurips-2017/</link><pubDate>Mon, 24 Jun 2024 15:59:28 +0800</pubDate><guid>https://lif323.github.io/posts/post-11/note-on-realtimeimagesaliency-neurips-2017/</guid><description>This paper proposes a fast saliency deletion method that can be applied to any differentiable image classifier. Their method provides explanations for model output by learning a mask $M$, where each element in $M$ represents the importance of elements in the input. The perturbation method is defined as follows: $$ \Phi(X, M) = X \odot M + A \odot (1 - M), $$ where $X$ is the original input, and $A$ is a reference input, which is usually a highly blurred version of $X$.</description></item><item><title>Note on Extremal Perturbation ICCV 2019</title><link>https://lif323.github.io/posts/post-10/note-on-extremal-perturbation-iccv-2019/</link><pubDate>Fri, 21 Jun 2024 16:16:47 +0800</pubDate><guid>https://lif323.github.io/posts/post-10/note-on-extremal-perturbation-iccv-2019/</guid><description>This paper is an improvement on meaningful perturbation(2017 Interpretable explanations of black boxes by meaningful perturbation). They reformulate the optimization problem of meaningful perturbation as follows: $$ m_{\lambda, \beta} = \argmax_{m} \Phi(m \otimes x) - \lambda \|m\|_{1} - \beta \mathcal{S}(m). $$ They believe that the meaning of the trade-off of this formulation is unclear. In particular, choosing different $\lambda$ and $\beta$ will result in different masks without a clear way of comparing them.</description></item><item><title>Cross Entropy</title><link>https://lif323.github.io/posts/post-8/cross_entropy/</link><pubDate>Thu, 20 Jun 2024 15:57:12 +0800</pubDate><guid>https://lif323.github.io/posts/post-8/cross_entropy/</guid><description>Cross-entropy is commonly used to quantify the difference between two probability distributions. In machine learning, its definition for one instance is as follows: $$ H(p, q) = - \sum_{c \in C} p(c)\log q(c) $$ The &amp;rsquo;true&amp;rsquo; distribution is usually expressed in terms of a one-hot distribution.
Suppose that the true label of an instance is B. The one-hot distribution for this instance is:
Pr(Class A) Pr(Class B) Pr(Class C) 0.0 1.</description></item><item><title>Note on MeaningfulPerturbation ICCV 2017</title><link>https://lif323.github.io/posts/post-7/note-on-meaningfulperturbation-iccv-2017/</link><pubDate>Tue, 18 Jun 2024 22:07:07 +0800</pubDate><guid>https://lif323.github.io/posts/post-7/note-on-meaningfulperturbation-iccv-2017/</guid><description>As far as I know, this paper is the first to propose providing explanations for black-box models by learning a mask. Additionally, this paper presents an interesting perspective, which views explaining black-box models as a form of meta-learning. Specifically, an explanation is a rule that predicts the block-box model&amp;rsquo;s output for a given input.
A significant advantage of formulating explanations as meta learning is that the fidelity of the explanations can be measured as prediction accuracy.</description></item><item><title>Note on ConceptBottleneckModels ICML 2020</title><link>https://lif323.github.io/posts/post-6/note-on-conceptbottleneckmodels-icml-2020/</link><pubDate>Fri, 14 Jun 2024 21:13:21 +0800</pubDate><guid>https://lif323.github.io/posts/post-6/note-on-conceptbottleneckmodels-icml-2020/</guid><description>In this paper, they systematically study different ways of learning concept bottleneck models. Let $L_{c_j}$ be a loss function that measures the discrepancy between the predicted and true j-th concept, and let $L_Y$ measure the discrepancy between predicted and true targets. They consider the following ways to learn a concept bottleneck model $(\hat{f}, \hat{g})$:
The independent bottleneck learns $\hat{f}$ and $\hat{g}$ independently: $\hat{f} = \argmin_{f}\sum_{i}L_{Y}(f(c^{i}); y^i)$, and $\hat{g} = \argmin_{g}\sum_{i,j}L_{C_j}(g_j(x^i);c_j^i)$. While $\hat{f}$ is trained using the true $c$, at test time it still takes $\hat{g}(x)$ as input.</description></item><item><title>Note on SaliencyGuidedTraining NeurIPS 2021</title><link>https://lif323.github.io/posts/post-5/note-on-saliencyguidedtraining-neurips-2021/</link><pubDate>Tue, 11 Jun 2024 16:08:08 +0800</pubDate><guid>https://lif323.github.io/posts/post-5/note-on-saliencyguidedtraining-neurips-2021/</guid><description>This paper proposes a saliency-guided training procedure aimed at reducing noisy gradients, which can lead to unfaithful feature attributions, while maintaining the predictive performance of the model.
During saliency-guided training, for every input $X$, they create a new input $\tilde{X}$ by masking the features with low gradient values as follows: $$ \tilde{X} = M_{k}(S(\nabla_{X}f_{\theta}(X), X), $$ where $S(\cdot)$ is a sorting function and $M(\cdot)$ replaces the bottom $k$ elements with a mask distribution based on the order provided by $S(\cdot)$.</description></item><item><title>Note on DynaMask ICML 2021</title><link>https://lif323.github.io/posts/post-4/</link><pubDate>Fri, 07 Jun 2024 22:36:34 +0800</pubDate><guid>https://lif323.github.io/posts/post-4/</guid><description>This paper is among the earliest works to learn a mask to measure the importance of each feature. Assuming a mask $M$, where $M_{t,i} \in [0, 1] $. When $M_{t, i} = 0$, this feature is irrelevant for black-box prediction. Conversely, when $M_{t, i}=1$, this feature is important for black-box prediction. The input $x$ is perturbed based on $M_{t, i}$. A simple perturbation method is as follows: $$ x'_{t,i} = x_{t,i} * M_{t, i} + b_{t,i} * (1 - M_{t, i}) $$ where $b_{t,i}$ is usually generated based on the data distribution.</description></item><item><title>Note on GSAT ICML 2022</title><link>https://lif323.github.io/posts/post-3/</link><pubDate>Thu, 30 May 2024 17:40:58 +0800</pubDate><guid>https://lif323.github.io/posts/post-3/</guid><description>This paper introduces GSAT methods for generating explanations in graph learning. GSAT is composed of the following parts:
$G = (A, X)$ $A$ is the adjacency matrix and $X$ is the node attributes. Let $V$ and $E$ denote the node set and the edge set.
Firstly, the extractor $g_{\phi}$ encodes the input graph $G$ via a GNN into a set of node representations $(h_u | u \in X)$. For any two nodes $v$ and $u$, an MLP layer maps the concatenation of their node representations to the probability $p_{u, v} \in [0, 1]$ of an edge existing between that corresponding pair of nodes.</description></item><item><title>Gumbel Softmax</title><link>https://lif323.github.io/posts/post-2/gumbel-softmax/</link><pubDate>Thu, 18 Apr 2024 11:24:57 +0800</pubDate><guid>https://lif323.github.io/posts/post-2/gumbel-softmax/</guid><description>What is Gubmel-softmax? Gumbel-softmax is an efficient gradient estimator for the non-differentiable sample from a categorical distribution.
Let $z$ be a categorical variable with class probabilities $\pi_1$, $\pi_2$, $\cdots$, $\pi_k$.
The Gumbel-Max trick is an efficient way to draw samples $z$ from a categorical distribution with class probabilities $\pi$: $$ z = \text{one\_hot}(\argmax_{i}\left[g_i + log \pi_i\right]) $$ where $g_1$, $\cdots$, $g_k$ are i.i.d samples drawn from Gumbel(0, 1). The Gubel(0, 1) distribution can be sampled by drawing $u \sim \text{Uniform}(0, 1)$ and computing $g = - \log (- \log (u))$.</description></item><item><title>Notes on the Paper NeurIPS 2023 Evaluating Post Hoc Explanations for Graph Neural Networks via Robustness Analysis</title><link>https://lif323.github.io/posts/post-1/notes-on-the-paper-neurips-2023-evaluating-post-hoc-explanations-for-graph-neural-networks-via-robustness-analysis/</link><pubDate>Sat, 13 Apr 2024 12:45:24 +0800</pubDate><guid>https://lif323.github.io/posts/post-1/notes-on-the-paper-neurips-2023-evaluating-post-hoc-explanations-for-graph-neural-networks-via-robustness-analysis/</guid><description>This paper focuses on the performance of evaluation methods of Post-hoc Explanations. Current prevailing evalution methods mainly inlcudes two types: Feature Removal methods and Generation-based methods.
This paper hones in on the evaluation of the effectiveness of Post-hoc Explanation evaluation methods. Predominantly, current methodologies fall into two primary classifications: Feature Removal and Generation-based approaches.
Feature Removal operates on the principle of excising salient features identified through explanation mechanisms. The merit of such explanations is quantified by the variance in the model&amp;rsquo;s output pre and post-feature extrication.</description></item></channel></rss>