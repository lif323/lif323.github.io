<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>lif323</title><link>https://lif323.github.io/</link><description>Recent content on lif323</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Wed, 25 Sep 2024 09:54:31 +0800</lastBuildDate><atom:link href="https://lif323.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>CoRTX ICLR 2023</title><link>https://lif323.github.io/posts/post-31/</link><pubDate>Wed, 25 Sep 2024 09:54:31 +0800</pubDate><guid>https://lif323.github.io/posts/post-31/</guid><description>&lt;p>Before introducing this work, I will first present the Real-time explainer (RTX) framework. RTX is a one-feed-forward explainer that can generate model explanations more efficiently. A major limitation of existing RTX approaches is their reliance on a large number of explanation labels. In my view, RTX is a neural network that generates explanations but like other domains, it also requires substantial data for training.&lt;/p>
&lt;p>However, due to limited computational resources and constrained human efforts. accurate explanation labels are difficult to obtain. To address this issue, thie paper proposes the Contrastive Real-Time eXplanation (CoRTX) method. CoRTX trains an encoder using contrastive learning to learn latent explanations in a self-supervised manner, thereby allevating the challenge of data scarcity.&lt;/p></description></item><item><title>VeriX (Verified eXplainability)</title><link>https://lif323.github.io/posts/post-30/</link><pubDate>Thu, 19 Sep 2024 16:35:29 +0800</pubDate><guid>https://lif323.github.io/posts/post-30/</guid><description>&lt;p>This paper presents VeriX, a tool for generating optimal robust explanations and counterfactuals along decision boundaries of deep neural networks.&lt;/p>
&lt;p>Specifically, its perturbations are bounded.&lt;/p>
&lt;p>The pseudocode is shown below.&lt;/p>
&lt;p>&lt;img loading="lazy" src="image.png" alt="alt text" />
&lt;/p>
&lt;p>Overall, VeriX iterates through each feature in $\mathbf{x}$, checking whether the feature satisfies the following condition: a bounded perturbation on $\mathbf{B} \cup \{i\}$ dose not alter the model&amp;rsquo;s prediction. If the condition is met, the feature is considered unimportant; otherwise, it is deemed important. In my view, while VeriX successfully identifies important feature, it does not appear to provide an importance score for each feature.&lt;/p></description></item><item><title>MAE: Masked Autoencoder CVPR 2021</title><link>https://lif323.github.io/posts/post-29/</link><pubDate>Wed, 18 Sep 2024 10:38:12 +0800</pubDate><guid>https://lif323.github.io/posts/post-29/</guid><description>&lt;p>This paper proposes a scalable self-supervised learning method-Masked Autoencoders (MAE) for computer vision. MAE reconstructs the missing pixels from randomly masked images. MAE has two key design elements.&lt;/p>
&lt;p>Firstly, the encoder-decoder architecture is asymmetric. The encoder processes only the visible parts of the image, ignoring the masked pixels.
However, the decoder&amp;rsquo;s input includes not only the encoded visiable pixels but also the masked tokens. Specifically, the masked tokens are shared, learnable vectors that represent the missing pixels to predicted. Notably, the decoder is lightweight.&lt;/p></description></item><item><title>Moco CVPR 2020</title><link>https://lif323.github.io/posts/post-28/</link><pubDate>Mon, 16 Sep 2024 17:03:35 +0800</pubDate><guid>https://lif323.github.io/posts/post-28/</guid><description>&lt;p>This paper proposes a Momentum Contrast (MoCo) method for unsupervised visual representation learning. Unlike previous methods, MoCo treats contrastive learning as a dictionary loolup task. The framework of MoCo is outlined as follows:
&lt;img loading="lazy" src="image.png" alt="alt text" />
&lt;/p>
&lt;p>Moco trains the visual representation encoder by matching the encoder query $q$ with keys encoded in a dictionary. Notably, the encoder is optimized via backpropagation, while the momentum encoder is optimized using the momentum update. The momentum update is defined as follows:
&lt;/p></description></item><item><title>DiET NeurIPS 2023</title><link>https://lif323.github.io/posts/post-27/</link><pubDate>Sat, 14 Sep 2024 21:29:31 +0800</pubDate><guid>https://lif323.github.io/posts/post-27/</guid><description>&lt;p>A major issue with post-hoc explainability is its inability to faithfully represent the model&amp;rsquo;s underlying decision-making process.
This primarily arises from the fact that post-hoc methods typically generate explanations using perturbation techniques, which create perturbed instances by altering the features of an instance, potentially pushing them outside the original data distribution.&lt;/p>
&lt;p>Two approaches can address this issue.
One approach involving ensure that perturbed instances remain within the original data distribution, typically by leveraging generative models.
The other approach requires the model&amp;rsquo;s predictions to remain unchanged when unimportant features in the instance are perturbed.
This paper adopts the latter approach.&lt;/p></description></item><item><title>RISE: Randomized Input Sampling for Explanation BMVC 2018</title><link>https://lif323.github.io/posts/post-26/</link><pubDate>Thu, 05 Sep 2024 22:34:19 +0800</pubDate><guid>https://lif323.github.io/posts/post-26/</guid><description>&lt;p>The key idea of RISE to measure the importance of an image region is to obscure or &amp;lsquo;perturb&amp;rsquo; it and observe how much this affects the black box decision.&lt;/p>
&lt;h1 id="rise">RISE&lt;/h1>
&lt;p>Assume that $\mathbf{M}:\Lambda \rightarrow \{0, 1\}$ is a binary mask with distribution $\mathcal{D}$. They consider the confidence score of perturbed input as the random variable $f(\mathbf{I} \odot \mathbf{M})$. They define importance of pixel $\lambda$ as the expected score over all possible masks $\mathbf{M}$ over all possible masks $\mathbf{M}$ conditioned on the event that pixel $\lambda$ is observed, i.e. $\mathbf{M}({\lambda}) = 1$:
&lt;/p></description></item><item><title>Frequency masking arXiv 2024</title><link>https://lif323.github.io/posts/post-25/</link><pubDate>Thu, 05 Sep 2024 16:37:03 +0800</pubDate><guid>https://lif323.github.io/posts/post-25/</guid><description>&lt;p>This paper proposes a method called FreqRISE. Current apporaches assume that salient information resides in the time domain (the raw input space), they argue that this assumption is less reasonable and that salient information is more likely to reside in the frequency domain.&lt;/p>
&lt;p>Traditional mask-based methods use a mask $\mathbf{M}$ to occlude features in the input space by performing element-wise multiplication:
:
&lt;/p>
$$
\hat{\mathbf{X}} = \mathbf{X} \odot \mathbf{M}.
$$&lt;p>
Then, they observe the changes in the output of the model $f$.
&lt;/p></description></item><item><title>Maximum Likelihood Estimation and KL Divergence</title><link>https://lif323.github.io/posts/post-24/</link><pubDate>Wed, 04 Sep 2024 10:43:33 +0800</pubDate><guid>https://lif323.github.io/posts/post-24/</guid><description>&lt;p>The core idea of maximum likelihood estimation is to assume that we draw samples $\{x^1, x^2, \cdots, x^m\}$ from the data distribution $P_{\text{data}}(x)$ and calculate the probability $P_{\theta}(x^i)$ of observing each sample $x^i$. The objective is to find the parameters $\theta$ that maximize the likelihood of observing all the samples. The optimization objective is formulated as follows:&lt;/p>
$$
\theta^{*} = \argmax_{\theta} \prod_{i=1}^{m} P_{\theta}(x^i)
$$&lt;p>Now, we will explore the relationship between maximum likelihood estimation and KL divergence.
&lt;/p></description></item><item><title>CEM NeurIPS 2018</title><link>https://lif323.github.io/posts/post-20/</link><pubDate>Mon, 26 Aug 2024 10:44:24 +0800</pubDate><guid>https://lif323.github.io/posts/post-20/</guid><description>&lt;p>This paper proposes a Contrastive Explanations method(CEM).
They define a type of explanation where, given an input, the goal is to identify which features are both minimal and sufficient to justify its classification as well as which features are minimal and necessarily absent.&lt;/p>
&lt;p>&lt;strong>Pertinent Negatives (PN)&lt;/strong> refers to the importance of missing features in model predictions. It essentially represents a counterfactual explanation. $\mathbf{x}_0 + \boldsymbol{\delta}$ denotes the counterfactual instance. A pertinent negative can be identified by solving the following optimization problem:&lt;/p></description></item><item><title>Note Integrated Directional Gradients</title><link>https://lif323.github.io/posts/post-17/note-integrated-directional-gradients/</link><pubDate>Mon, 05 Aug 2024 16:07:54 +0800</pubDate><guid>https://lif323.github.io/posts/post-17/note-integrated-directional-gradients/</guid><description>&lt;p>This paper proposes the Integrated Directional Gradients (IDG) to computing feature group attribution. One important thing is to find the family of meaningful feature subsets which is defined by the domain related methods, such as the constituency parse tree for NLP.&lt;/p>
&lt;p>In IDG, the importance of a subset of features is the path integral of the directional gradient over the straight line path from the baseline $b$ to the input $x$. The process of computing the importance $v(S)$ of a subset of features is defined as follows:
Firstly, they compute the difference vector $z^S$:
&lt;/p></description></item><item><title>Note Ranking Problems</title><link>https://lif323.github.io/posts/post-16/</link><pubDate>Sun, 14 Jul 2024 09:09:56 +0800</pubDate><guid>https://lif323.github.io/posts/post-16/</guid><description>&lt;p>In machine learning, most problems are classified as Classification or Regression. However, this categorization is not always suitable for real-world problems. In some tasks, the classes are imbalanced but exhibit some weak order. In these cases, the loss functions for Classification and Regression do not perform well. This blog will apply the less commonly used margin ranking loss to address this issue.&lt;/p>
&lt;p>Here is an example from the UCI Wine Quality Dataset from Cortez et al. Suppose we are a wine supplier and we find that a high review in some wine magazines increases the demand for a particular wine. Therefore, we want to predict the review score based on some measurable features of the wine. This way, we can increase the stock of the corresponding wine in advance, before the demand rises.&lt;/p></description></item><item><title>Note Discretized Integrated Gradients EMNLP 2021</title><link>https://lif323.github.io/posts/post-15/</link><pubDate>Mon, 08 Jul 2024 15:58:29 +0800</pubDate><guid>https://lif323.github.io/posts/post-15/</guid><description>&lt;p>This paper proposes the Discretized Integrated Gradients (DIG) method, which effectively applies the Integrated Gradients(IG) method to the word embedding space.&lt;/p>
&lt;p>Integrated Gradient (IG) measures the importance of features by using the average of model gradients at interpolation points along a straight path in the input space.&lt;/p>
&lt;p>However, in the word embedding space, due to its discrete nature, interpolation points may not accurately represent textual data. In particular cases, interpolation points could be outliers.&lt;/p></description></item><item><title>Note Learning Perturbations ICML 2023</title><link>https://lif323.github.io/posts/post-14/note-learningperturbations-2023/</link><pubDate>Sun, 07 Jul 2024 15:00:05 +0800</pubDate><guid>https://lif323.github.io/posts/post-14/note-learningperturbations-2023/</guid><description>&lt;p>This paper improves existing methods that provide explanations using trainable masks.
Inspired by methods for static data, the current approach uses fixed perturbations. However, this paper argues that this approach may not be suitable for time series data.
In this paper, their method not only has a trainable mask but also trainable perturbations.&lt;/p>
&lt;p>Existing methods using fixed perturbations can be expressed as follows:
&lt;/p>
$$
\Phi(\mathbf{x}, \mathbf{m}) = \mathbf{m} \times \mathbf{x} + (1 - \mathbf{m}) \times g(\mathbf{x}),
$$&lt;p>
where $g(\mathbf{x})$ is a function of the input. A possible definition is $g(\mathbf{x}) = \frac{1}{W}\sum_{t'=t-W}^{t}x_{t'}$.&lt;/p></description></item><item><title>KL Divergence and Cross Entropy</title><link>https://lif323.github.io/posts/post-12/kl-divergence-and-cross-entropy/</link><pubDate>Mon, 24 Jun 2024 21:37:58 +0800</pubDate><guid>https://lif323.github.io/posts/post-12/kl-divergence-and-cross-entropy/</guid><description>&lt;p>The defination of Cross-entropy is as follows:
&lt;/p>
$$
H(P, Q) = - \sum_{x}p(x)\log Q(x)
$$&lt;p>The KL divergence is defined as follows:
&lt;/p>
$$
KL(P|Q) = \sum_{x}P(x)\log\frac{P(x)}{Q(x)}
$$&lt;p>We first introduce the definition of information entropy:
&lt;/p>
$$
S(v) = - \sum_{i}p(v_i)\log p(v_i),
$$&lt;p>
where $p(v_i)$ represents the probability of state $v_i$. From the perspective of information theory, $S(v)$ is the information required to remove system uncertainty.&lt;/p>
&lt;p>The formula for KL divergence can be further transformed into the following form:
&lt;/p></description></item><item><title>Note on RealTimeImageSaliency NeurIPS 2017</title><link>https://lif323.github.io/posts/post-11/note-on-realtimeimagesaliency-neurips-2017/</link><pubDate>Mon, 24 Jun 2024 15:59:28 +0800</pubDate><guid>https://lif323.github.io/posts/post-11/note-on-realtimeimagesaliency-neurips-2017/</guid><description>&lt;p>This paper proposes a fast saliency deletion method that can be applied to any differentiable image classifier.
Their method provides explanations for model output by learning a mask $M$, where each element in $M$ represents the importance of elements in the input.
The perturbation method is defined as follows:
&lt;/p>
$$
\Phi(X, M) = X \odot M + A \odot (1 - M),
$$&lt;p>
where $X$ is the original input, and $A$ is a reference input, which is usually a highly blurred version of $X$.&lt;/p></description></item><item><title>Note on Extremal Perturbation ICCV 2019</title><link>https://lif323.github.io/posts/post-10/note-on-extremal-perturbation-iccv-2019/</link><pubDate>Fri, 21 Jun 2024 16:16:47 +0800</pubDate><guid>https://lif323.github.io/posts/post-10/note-on-extremal-perturbation-iccv-2019/</guid><description>&lt;p>This paper is an improvement on meaningful perturbation(2017 Interpretable explanations of black boxes by meaningful perturbation).
They reformulate the optimization problem of &lt;strong>meaningful perturbation&lt;/strong> as follows:
&lt;/p>
$$
m_{\lambda, \beta} = \argmax_{m} \Phi(m \otimes x) - \lambda \|m\|_{1} - \beta \mathcal{S}(m).
$$&lt;p>They believe that the meaning of the trade-off of this formulation is unclear.
In particular, choosing different $\lambda$ and $\beta$ will result in different masks without a clear way of comparing them.&lt;/p></description></item><item><title>Cross Entropy</title><link>https://lif323.github.io/posts/post-8/cross_entropy/</link><pubDate>Thu, 20 Jun 2024 15:57:12 +0800</pubDate><guid>https://lif323.github.io/posts/post-8/cross_entropy/</guid><description>&lt;p>Cross-entropy is commonly used to quantify the difference between two probability distributions.
In machine learning, its definition for one instance is as follows:
&lt;/p>
$$
H(p, q) = - \sum_{c \in C} p(c)\log q(c)
$$&lt;p>The &amp;rsquo;true&amp;rsquo; distribution is usually expressed in terms of a one-hot distribution.&lt;/p>
&lt;p>Suppose that the true label of an instance is B. The one-hot distribution for this instance is:&lt;/p>
&lt;pre tabindex="0">&lt;code>Pr(Class A) Pr(Class B) Pr(Class C)
0.0 1.0 0.0
&lt;/code>&lt;/pre>&lt;p>Suppose a machine learning algorithm predicts the following probability distribution:&lt;/p></description></item><item><title>Note on MeaningfulPerturbation ICCV 2017</title><link>https://lif323.github.io/posts/post-7/note-on-meaningfulperturbation-iccv-2017/</link><pubDate>Tue, 18 Jun 2024 22:07:07 +0800</pubDate><guid>https://lif323.github.io/posts/post-7/note-on-meaningfulperturbation-iccv-2017/</guid><description>&lt;p>As far as I know, this paper is the first to propose providing explanations for black-box models by learning a mask. Additionally, this paper presents an interesting perspective, which views explaining black-box models as a form of meta-learning. Specifically, an explanation is a rule that predicts the block-box model&amp;rsquo;s output for a given input.&lt;/p>
&lt;p>A significant advantage of formulating explanations as meta learning is that the fidelity of the explanations can be measured as prediction accuracy.&lt;/p></description></item><item><title>Note on ConceptBottleneckModels ICML 2020</title><link>https://lif323.github.io/posts/post-6/note-on-conceptbottleneckmodels-icml-2020/</link><pubDate>Fri, 14 Jun 2024 21:13:21 +0800</pubDate><guid>https://lif323.github.io/posts/post-6/note-on-conceptbottleneckmodels-icml-2020/</guid><description>&lt;p>In this paper, they systematically study different ways of learning concept bottleneck models. Let $L_{c_j}$ be a loss function that measures the discrepancy between the predicted and true j-th concept, and let $L_Y$ measure the discrepancy between predicted and true targets. They consider the following ways to learn a concept bottleneck model $(\hat{f}, \hat{g})$:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>The &lt;em>independent bottleneck&lt;/em> learns $\hat{f}$ and $\hat{g}$ independently: $\hat{f} = \argmin_{f}\sum_{i}L_{Y}(f(c^{i}); y^i)$, and $\hat{g} = \argmin_{g}\sum_{i,j}L_{C_j}(g_j(x^i);c_j^i)$. While $\hat{f}$ is &lt;strong>trained using the true $c$, at test time it still takes $\hat{g}(x)$ as input&lt;/strong>.&lt;/p></description></item><item><title>Note on SaliencyGuidedTraining NeurIPS 2021</title><link>https://lif323.github.io/posts/post-5/note-on-saliencyguidedtraining-neurips-2021/</link><pubDate>Tue, 11 Jun 2024 16:08:08 +0800</pubDate><guid>https://lif323.github.io/posts/post-5/note-on-saliencyguidedtraining-neurips-2021/</guid><description>&lt;p>This paper proposes a saliency-guided training procedure aimed at reducing &lt;strong>noisy gradients&lt;/strong>, which can lead to unfaithful feature attributions, while maintaining the predictive performance of the model.&lt;/p>
&lt;p>During saliency-guided training, for every input $X$, they create a new input $\tilde{X}$ by masking the features with low gradient values as follows:
&lt;/p>
$$
\tilde{X} = M_{k}(S(\nabla_{X}f_{\theta}(X), X),
$$&lt;p>
where $S(\cdot)$ is a sorting function and $M(\cdot)$ replaces the bottom $k$ elements with a mask distribution based on the order provided by $S(\cdot)$.&lt;/p></description></item><item><title>Note on DynaMask ICML 2021</title><link>https://lif323.github.io/posts/post-4/</link><pubDate>Fri, 07 Jun 2024 22:36:34 +0800</pubDate><guid>https://lif323.github.io/posts/post-4/</guid><description>&lt;p>&lt;img loading="lazy" src="./icml-2020-dyna-mask.png" alt="alt text" />
This paper is among the earliest works to learn a mask to measure the importance of each feature. Assuming a mask $M$, where $M_{t,i} \in [0, 1] $. When $M_{t, i} = 0$, this feature is irrelevant for black-box prediction. Conversely, when $M_{t, i}=1$, this feature is important for black-box prediction.
The input $x$ is perturbed based on $M_{t, i}$. A simple perturbation method is as follows:
&lt;/p></description></item><item><title>Note on GSAT ICML 2022</title><link>https://lif323.github.io/posts/post-3/</link><pubDate>Thu, 30 May 2024 17:40:58 +0800</pubDate><guid>https://lif323.github.io/posts/post-3/</guid><description>&lt;p>&lt;img loading="lazy" src="./icml-2022-gsat.png" alt="alt text" />
&lt;/p>
&lt;p>This paper introduces GSAT methods for generating explanations in graph learning. GSAT is composed of the following parts:&lt;/p>
&lt;p>$G = (A, X)$ $A$ is the adjacency matrix and $X$ is the node attributes. Let $V$ and $E$ denote the node set and the edge set.&lt;/p>
&lt;p>Firstly, the extractor $g_{\phi}$ encodes the input graph $G$ via a GNN into a set of node representations $(h_u | u \in X)$. For any two nodes $v$ and $u$, an MLP layer maps the concatenation of their node representations to the probability $p_{u, v} \in [0, 1]$ of an edge existing between that corresponding pair of nodes.&lt;/p></description></item><item><title>Gumbel Softmax</title><link>https://lif323.github.io/posts/post-2/gumbel-softmax/</link><pubDate>Thu, 18 Apr 2024 11:24:57 +0800</pubDate><guid>https://lif323.github.io/posts/post-2/gumbel-softmax/</guid><description>&lt;h1 id="what-is-gubmel-softmax">What is Gubmel-softmax?&lt;/h1>
&lt;p>Gumbel-softmax is an efficient gradient estimator for the non-differentiable sample from a categorical distribution.&lt;/p>
&lt;p>Let $z$ be a categorical variable with class probabilities $\pi_1$, $\pi_2$, $\cdots$, $\pi_k$.&lt;/p>
&lt;p>The Gumbel-Max trick is an efficient way to draw samples $z$ from a categorical distribution with class probabilities $\pi$:
&lt;/p>
$$
z = \text{one\_hot}(\argmax_{i}\left[g_i + log \pi_i\right])
$$&lt;p>
where $g_1$, $\cdots$, $g_k$ are i.i.d samples drawn from Gumbel(0, 1).
The Gubel(0, 1) distribution can be sampled by drawing $u \sim \text{Uniform}(0, 1)$ and computing $g = - \log (- \log (u))$.&lt;/p></description></item><item><title>Notes on the Paper NeurIPS 2023 Evaluating Post Hoc Explanations for Graph Neural Networks via Robustness Analysis</title><link>https://lif323.github.io/posts/post-1/notes-on-the-paper-neurips-2023-evaluating-post-hoc-explanations-for-graph-neural-networks-via-robustness-analysis/</link><pubDate>Sat, 13 Apr 2024 12:45:24 +0800</pubDate><guid>https://lif323.github.io/posts/post-1/notes-on-the-paper-neurips-2023-evaluating-post-hoc-explanations-for-graph-neural-networks-via-robustness-analysis/</guid><description>&lt;p>This paper focuses on the performance of evaluation methods of Post-hoc Explanations. Current prevailing evalution methods mainly inlcudes two types: Feature Removal methods and Generation-based methods.&lt;/p>
&lt;p>This paper hones in on the evaluation of the effectiveness of Post-hoc Explanation evaluation methods. Predominantly, current methodologies fall into two primary classifications: Feature Removal and Generation-based approaches.&lt;/p>
&lt;p>Feature Removal operates on the principle of excising salient features identified through explanation mechanisms. The merit of such explanations is quantified by the variance in the model&amp;rsquo;s output pre and post-feature extrication. The larger the resultant decrement in model performance, which intimates the importance of the deleted features to the model&amp;rsquo;s functionality, the more adept the explanation method is deemed.&lt;/p></description></item></channel></rss>